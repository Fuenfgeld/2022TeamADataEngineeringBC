{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PySparkTutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "FjsR9GZemXN9",
        "FSLy8EIT9Qs1",
        "AvnZOVxV7K9t",
        "o_9RPA4ZL7W9",
        "Ef25TfNJkhfs"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fuenfgeld/2022TeamADataEngineeringBC/blob/main/PySparkTutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##0. Data Engineering Bootcamp\n",
        "In this tutorial, you will be introduced to an aspect of Data Engineering called ETL. Together we will implement an ETL workflow with Apache Spark in Python. By the end of the tutorial, you will be able to adapt such a workflow to your specific needs and know the benefits of using Spark in doing so."
      ],
      "metadata": {
        "id": "-DZ60lbz97U4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.1 What is Data Engineering ?\n",
        "\n",
        "Data engineering is the practice of designing and building systems for collecting, storing, and analyzing data at scale. Data engineers work in a variety of settings to build systems that collect, manage, and convert raw data into usable information for data scientists and business analysts to interpret. Their ultimate goal is to make data accessible so that organizations can use it to evaluate and optimize their performance. This last sentence also sums up the difference between a data engineer and a data analyst, whereas the former manages the data resources, the latter exploits them to gain valuable insights.\n",
        "\n",
        "### 0.2 What is ETL ?\n",
        "\n",
        "According to IBM, ETL, which stands for extract, transform and load, is a data integration process that combines data from multiple data sources into a single, consistent data store. It is closely linked with the concept of a *Data Warehouse* that describes central repositories of integrated data from one or more disparate sources. \n",
        "\n",
        "#### Extraction\n",
        "During data extraction, raw data is copied or exported from source locations from a variety of data sources, which can be structured or unstructured such as SQL databases, JSON files or even web pages.\n",
        "#### Transformation\n",
        "The collected raw data then undergoes data processing. Here, the data is transformed and consolidated for its intended analytical use case. Steps taken during transformation are de-duplicating values, performing calculations, translations, or summarizations based on the raw data and changing the shape of the data via joining and grouping operation in order to match the schema of the target data warehouse. The environment in which the transformation step is performed is also called *staging area*.\n",
        "#### Loading\n",
        "In this last step, the transformed data is moved from the staging area into a target data warehouse. Typically, this involves an initial loading of all data, followed by periodic loading of incremental data changes \n",
        "\n",
        "### 0.3 What is Spark ?\n",
        "\n",
        "According to the official website\n",
        "\n",
        ">*Apache Spark is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters.*\n",
        "\n",
        "Now, what does that mean ? You can think of Spark as a programming library that allows you to outsource your data engineering workflow to a set of servers (cluster) which enables you to parallelize operations, enabling faster execution and the ability to work with amounts of data that couldn't be handled on a single computer (Big Data). \n",
        "\n",
        "Hence, what Spark does is managing the interaction between your local  node (computer) and each node (server) of the cluster. Since Spark was originally written in Scala, there is no direct way to access its functionality in Python. This is where *PySpark* comes into play. You can think of PySpark as a Python-based wrapper on top of the Scala API. There are also similar wrappers for *R* and other programming languages, this is why the official website describes Spark as a *multi-language engine*. \n",
        "\n",
        "#### SparkSQL\n",
        "\n",
        "Although the *Resilient Distributed Dataset* (RDD) is the  fundamental data structure on which all higher-level data structures are constructed in Spark, this tutorial is going to focus on the *DataFrame* from the SparkSQL model which deals with structured data such as .JSON and .csv files. \n",
        "\n",
        "The DataFrame has two big advantages over the RDD. First it has  significant performance benefits over RDDs due to a powerful optimization engine and secondly important data science module such as *spark.streaming* an *spark.ml* work with DataFrames instead of RDDs.\n",
        " \n",
        "## 0.4. Conclusion.\n",
        "You learned what data engineering is, how an ETL workflow is structured and what role Spark plays in such a  context. Now let's get started with coding stuff. \n",
        "\n",
        "The next three lines of code will make Pyspark and all the relevant data you need to finish this tutorial available in your Colab notebook.\n"
      ],
      "metadata": {
        "id": "tidhxKfZlyJz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kMaqIMwKk-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f901159-9f6b-4443-d8cd-b57d14bd6b73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 29 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Collecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 51.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=634baa300c8d7b65a45b498a016dd1801e7ef9eaf508a3d5955c2f476774e2eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -cq https://raw.githubusercontent.com/Fuenfgeld/2022TeamADataEngineeringBC/ca4b2ecc9e9ee242037d11c27edd4f4ad770e7ee/iris.json"
      ],
      "metadata": {
        "id": "AWFXHurvlAP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -cq https://raw.githubusercontent.com/Fuenfgeld/2022TeamADataEngineeringBC/main/iris2.json"
      ],
      "metadata": {
        "id": "UwhFqG5OpkrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Getting Started \n",
        "\n",
        "Let's get started building our first Spark application.At the core of our Spark application is the *SparkSession* object,which acts as a point of entry to interact with underlying Spark\n",
        "functionalities.\n",
        "\n",
        "Usually, Spark would delegate the computation jobs to the so-called *executors* (CPUs in each node of the cluster) through the so-called *driver*.Since this notebook isn't connected to a cluster,the jobs will be performed locally, though.The concept however is still the same,just be aware that we aren't exploiting Sparks full capabilities here for practical reasons (Clusters are not trivial to deal with).\n"
      ],
      "metadata": {
        "id": "FjsR9GZemXN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "WQMsUJrIM3-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our Spark session is initialized we can load in some data to work with. Spark can handle data from all kinds of sources such as .json files, .csv files and even access data from AWS or Azure via dedicated interfaces."
      ],
      "metadata": {
        "id": "ExNrRpo4-HYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Schemas and Creating DataFrames\n",
        "The core object of pyspark.sql is the *DataFrame* which stores the information obtained from external sources (.JSON, .csv files etc.) or created by the user within Python. To the humans eye the DataFrame behaves like a table, computationally however it is a more complex structure due to the distributed nature of spark objects.\n",
        "\n",
        "Before we load in our first dataset in such a DataFrame we should talk about so-called *schemas*. When reading in structured data from an external source, it is often known to the user how the data is structured, for instance what the names and data types of the columns are. In contrast to the user however, Spark doesn't have any idea about what the types and columns are going to look like.\n",
        "\n",
        "Defining and including a schema in the data loading pipeline in line `3` makes Spark aware of the structure of the data before loading it. This has three big advantages:\n",
        "\n",
        "1.   You relieve Spark from the onus of inferring data types.\n",
        "\n",
        "2.   You prevent Spark from creating a separate job just to read a large portion of your file to ascertain the schema, which for a large data file can be expensive and time-consuming.\n",
        "\n",
        "3. You can detect errors early if data doesn’t match the schema.\n",
        "\n",
        "It is therefore encouraged to define a schema upfront, especially when dealing with large sources of data. One easy way to define a schema in PySpark is via a so-called *Data Definition Language* (DDL) string, as shown in line `1`.\n",
        "\n",
        " Since  `iris.json` is a large file with multiple lines, we also want to inform PySpark about this fact by adding `.option(\"multiline\",True)` to the pipeline. Although the second option in the pipeline instructing PySpark to make a header isn't necessary for .JSON files, it is for .csv files, and we include it for the sake of completeness."
      ],
      "metadata": {
        "id": "zGqOCdfwA36U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schema = \"petalLength DOUBLE, petalWidth DOUBLE, sepalLength DOUBLE, sepalWidth DOUBLE, species STRING\"\n",
        "\n",
        "df1 = (spark.read.option(\"multiline\",True)\n",
        "                 .option(\"header\",True)\n",
        "                 .schema(schema)\n",
        "                 .json('iris.json'))\n",
        "\n",
        "print(f\"Object Type: {type(df1)}\\n\")\n",
        "print(\"Column Info:\")\n",
        "df1.printSchema()\n",
        "print(\"Summary Statistics of columns:\")\n",
        "df1.describe().show()\n",
        "print(\"Overview Dataframe:\")\n",
        "df1.show(10)"
      ],
      "metadata": {
        "id": "Kc5OYn6VNa9S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1252ad02-d107-4c28-a4ab-c4340f74d881"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object Type: <class 'pyspark.sql.dataframe.DataFrame'>\n",
            "\n",
            "Column Info:\n",
            "root\n",
            " |-- petalLength: double (nullable = true)\n",
            " |-- petalWidth: double (nullable = true)\n",
            " |-- sepalLength: double (nullable = true)\n",
            " |-- sepalWidth: double (nullable = true)\n",
            " |-- species: string (nullable = true)\n",
            "\n",
            "Summary Statistics of columns:\n",
            "+-------+------------------+------------------+------------------+-------------------+---------+\n",
            "|summary|       petalLength|        petalWidth|       sepalLength|         sepalWidth|  species|\n",
            "+-------+------------------+------------------+------------------+-------------------+---------+\n",
            "|  count|               150|               150|               150|                150|      150|\n",
            "|   mean|3.7580000000000027| 1.199333333333334| 5.843333333333335|  3.057333333333334|     null|\n",
            "| stddev|1.7652982332594662|0.7622376689603467|0.8280661279778637|0.43586628493669793|     null|\n",
            "|    min|               1.0|               0.1|               4.3|                2.0|   setosa|\n",
            "|    max|               6.9|               2.5|               7.9|                4.4|virginica|\n",
            "+-------+------------------+------------------+------------------+-------------------+---------+\n",
            "\n",
            "Overview Dataframe:\n",
            "+-----------+----------+-----------+----------+-------+\n",
            "|petalLength|petalWidth|sepalLength|sepalWidth|species|\n",
            "+-----------+----------+-----------+----------+-------+\n",
            "|        1.4|       0.2|        5.1|       3.5| setosa|\n",
            "|        1.4|       0.2|        4.9|       3.0| setosa|\n",
            "|        1.3|       0.2|        4.7|       3.2| setosa|\n",
            "|        1.5|       0.2|        4.6|       3.1| setosa|\n",
            "|        1.4|       0.2|        5.0|       3.6| setosa|\n",
            "|        1.7|       0.4|        5.4|       3.9| setosa|\n",
            "|        1.4|       0.3|        4.6|       3.4| setosa|\n",
            "|        1.5|       0.2|        5.0|       3.4| setosa|\n",
            "|        1.4|       0.2|        4.4|       2.9| setosa|\n",
            "|        1.5|       0.1|        4.9|       3.1| setosa|\n",
            "+-----------+----------+-----------+----------+-------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might have wondered why we called `.show()` behind `.describe()` in line 6, especially if you are familiar with *Pandas*. The reason is the so called *Lazy Execution* where code is only executed  once so called *actions* are called. The next chapter will begin introducing these concepts in more depth. We will also get to know *transformations*, code that changes the structure and entries of DataFrames."
      ],
      "metadata": {
        "id": "s8_T1h5sC_Sk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Actions and Basic Transformations\n",
        "\n",
        "Spark operations on distributed data can be classified into two types: *transformations* and *actions*. Transformations, as the name suggests, transform a Spark DataFrame into a new DataFrame without altering the original data, giving it the property of *immutability*. \n",
        "\n",
        "\n",
        "All transformations are evaluated lazily. That is, their results are not computed immediately, but they are recorded or remembered as a *lineage*. A recorded lineage allows Spark, at a later time in its execution plan, to rearrange certain transformations or optimize them into stages for more efficient execution. \n",
        "\n",
        "Because Spark records each transformation in its lineage and the DataFrames are immutable between transformations, it can reproduce its original state by simply replaying the recorded lineage, giving it resiliency in the event of failures.\n",
        "\n",
        "Now let's set up some simple Data Engineering workflows in PySpark using actions and transformations."
      ],
      "metadata": {
        "id": "92lmS82aSNDx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Accessing Rows and Columns\n",
        "\n",
        "Since Spark was conceived to work with distributed data, there is no simple way to access rows at will.\n",
        "\n",
        "If you want to do so anyway, you have the possibility to pull the data onto your local node.\n",
        "\n",
        "The action `.collect()` collects the distributed data to the driver side as local data in Python. Note that this can throw an out-of-memory error when the dataset is too large to fit in the driver side because it collects all the data from executors to the driver side.\n",
        "\n",
        "Other actions to get only a subset of the  data to your driver's side are `.take()` and `.sample()`."
      ],
      "metadata": {
        "id": "FSLy8EIT9Qs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns list of Row objects\n",
        "local_df1 = df1.collect()\n",
        "print(f\"Type of entries: {type(local_df1[0])}\\n\")\n",
        "print(f\"Entries: {local_df1[:5]}\")"
      ],
      "metadata": {
        "id": "wqjHubRZSVJg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f33a5be5-f0e6-4a5a-e3e5-cd6e90fe0480"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of entries: <class 'pyspark.sql.types.Row'>\n",
            "\n",
            "Entries: [Row(petalLength=1.4, petalWidth=0.2, sepalLength=5.1, sepalWidth=3.5, species='setosa'), Row(petalLength=1.4, petalWidth=0.2, sepalLength=4.9, sepalWidth=3.0, species='setosa'), Row(petalLength=1.3, petalWidth=0.2, sepalLength=4.7, sepalWidth=3.2, species='setosa'), Row(petalLength=1.5, petalWidth=0.2, sepalLength=4.6, sepalWidth=3.1, species='setosa'), Row(petalLength=1.4, petalWidth=0.2, sepalLength=5.0, sepalWidth=3.6, species='setosa')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Accessing columns doesn't come with the difficulties associated with handling rows. If we want to get specific columns we can simply do so through the `.select()` method. "
      ],
      "metadata": {
        "id": "ClcImNjp1pRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.select(\"petalLength\").show(5)"
      ],
      "metadata": {
        "id": "YjXEYeYSPu6J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c97649c0-31f7-41c1-a216-87de2a6bb4e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|petalLength|\n",
            "+-----------+\n",
            "|        1.4|\n",
            "|        1.4|\n",
            "|        1.3|\n",
            "|        1.5|\n",
            "|        1.4|\n",
            "+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Adding and Removing  Columns\n",
        "\n",
        "In case we want to add columns we can do so via the `.withColumn()` method. Note that we have to specify the name of the column which is in this case `petalSum`. Usually, the new column is a function of one or more of the old columns. "
      ],
      "metadata": {
        "id": "AvnZOVxV7K9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_extraCol = df1.withColumn('newColumn', df1.petalWidth + df1.petalLength)\n",
        "df_extraCol.show(5)"
      ],
      "metadata": {
        "id": "7XRQiJVSij7m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "341fb1df-542e-4ae6-9af1-d6531f512376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+-----------+----------+-------+------------------+\n",
            "|petalLength|petalWidth|sepalLength|sepalWidth|species|         newColumn|\n",
            "+-----------+----------+-----------+----------+-------+------------------+\n",
            "|        1.4|       0.2|        5.1|       3.5| setosa|1.5999999999999999|\n",
            "|        1.4|       0.2|        4.9|       3.0| setosa|1.5999999999999999|\n",
            "|        1.3|       0.2|        4.7|       3.2| setosa|               1.5|\n",
            "|        1.5|       0.2|        4.6|       3.1| setosa|               1.7|\n",
            "|        1.4|       0.2|        5.0|       3.6| setosa|1.5999999999999999|\n",
            "+-----------+----------+-----------+----------+-------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The name `'newColumn'` isn't really informative. It's therefore hard for the user to deduce that is it the sum of `'petalWidth'` and `'petalLength'`. So why not rename it to something more indicative ? We can do this via the `.withColumnRenamed()` method."
      ],
      "metadata": {
        "id": "Pf1MnUAcI7M5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_extraCol = df_extraCol.withColumnRenamed('newColumn','petalSum')\n",
        "df_extraCol.show(5)"
      ],
      "metadata": {
        "id": "AHeB_op4JtMF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "578ef4b8-1ac7-4219-be30-e8eb7830fac5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+-----------+----------+-------+------------------+\n",
            "|petalLength|petalWidth|sepalLength|sepalWidth|species|          petalSum|\n",
            "+-----------+----------+-----------+----------+-------+------------------+\n",
            "|        1.4|       0.2|        5.1|       3.5| setosa|1.5999999999999999|\n",
            "|        1.4|       0.2|        4.9|       3.0| setosa|1.5999999999999999|\n",
            "|        1.3|       0.2|        4.7|       3.2| setosa|               1.5|\n",
            "|        1.5|       0.2|        4.6|       3.1| setosa|               1.7|\n",
            "|        1.4|       0.2|        5.0|       3.6| setosa|1.5999999999999999|\n",
            "+-----------+----------+-----------+----------+-------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to get rid of our new column `.drop()` can be used. In contrast to `.select()`, this method removes the specified column completely instead of returning it as slice ot the table.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NKioCZzjlL-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df_extraCol.drop(df_extraCol.petalSum)\n",
        "df1.show(5)"
      ],
      "metadata": {
        "id": "Xp8ScsA-ixVF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f2e2151-83ca-4815-bcc2-a435a0022988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+-----------+----------+-------+\n",
            "|petalLength|petalWidth|sepalLength|sepalWidth|species|\n",
            "+-----------+----------+-----------+----------+-------+\n",
            "|        1.4|       0.2|        5.1|       3.5| setosa|\n",
            "|        1.4|       0.2|        4.9|       3.0| setosa|\n",
            "|        1.3|       0.2|        4.7|       3.2| setosa|\n",
            "|        1.5|       0.2|        4.6|       3.1| setosa|\n",
            "|        1.4|       0.2|        5.0|       3.6| setosa|\n",
            "+-----------+----------+-----------+----------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. Basic Data Cleaning\n",
        "\n",
        "Just as in the hospital, hygiene is of great importance to working with data,sometimes rows contain entries that make dealing with our data more difficult or lower its quality (information pollution).Two examples come to mind: Duplicate entries could bias introduce into our data,which negatively impacts the performance of a lot of machine learning algorithms.\n",
        "\n",
        "The second example would be null entries, which might render some rows useless due to the fact that most algorithms generally can't handle such entries. Luckily, PySpark provides us with two methods `.dropna()` and `.dropDuplicates()` to get rid of such problematic rows.\n"
      ],
      "metadata": {
        "id": "o_9RPA4ZL7W9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(df1.select(\"petalWidth\")\n",
        "    .withColumnRenamed(\"petalWidth\", \"pW\")\n",
        "    .dropna()\n",
        "    .dropDuplicates()\n",
        "    .show(n=5))"
      ],
      "metadata": {
        "id": "22OiPKXUTVVd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e36cd047-b9cb-4fdd-b4e0-fc967bce1da9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| pW|\n",
            "+---+\n",
            "|2.4|\n",
            "|0.2|\n",
            "|1.4|\n",
            "|1.7|\n",
            "|2.3|\n",
            "+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although our dataframe is now free of unwanted entries we might still want to put further restrictions on the data we want to keep. "
      ],
      "metadata": {
        "id": "Em-5JUVf82Kg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4. Conditional Selection of Rows.\n",
        "\n",
        "In 2.1. We explained that directly accessing rows of a DataFrame comes with some caveats, it is however possible to indirectly access rows without pulling all the data onto your local node. This is done via conditional selection, where we select rows based on user given conditions via the `.filter()` method. This means however that we don't know which rows we will obtain in the end, hence why we speak of indirect access.\n",
        "\n",
        "Let's say we want to get only the flowers of type `\"virginica\"` we then have to write the following:"
      ],
      "metadata": {
        "id": "RtB5fpk0flQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(df1.select(\"species\", \"petalWidth\")\n",
        "    .filter(df1.species == \"virginica\")\n",
        "    .dropDuplicates()\n",
        "    .show(5))             "
      ],
      "metadata": {
        "id": "XN6A0o-ziDhm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81093465-a48a-4b38-f7ba-52ad278628dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+\n",
            "|  species|petalWidth|\n",
            "+---------+----------+\n",
            "|virginica|       2.2|\n",
            "|virginica|       1.7|\n",
            "|virginica|       2.5|\n",
            "|virginica|       2.4|\n",
            "|virginica|       1.6|\n",
            "+---------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also filter based on multiple conditions. In the next example we will use the alias `.where()` instead of `.filter()`. Note that both aliases are used, with `.where()` coming from SQL syntax."
      ],
      "metadata": {
        "id": "fNZTni8DBha5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(df1.select(\"species\", \"petalWidth\", \"petalLength\")\n",
        "    .where((df1.species == \"setosa\") & (df1.petalLength > 1.3))\n",
        "    .withColumn(\"petalSum\", df1.petalWidth + df1.petalLength)\n",
        "    .dropDuplicates()\n",
        "    .describe()\n",
        "    .show(5))"
      ],
      "metadata": {
        "id": "_K0KUmDVDQgc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f482533d-427b-4a47-a3ad-d027b3b1b99a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-------------------+-------------------+-------------------+\n",
            "|summary|species|         petalWidth|        petalLength|           petalSum|\n",
            "+-------+-------+-------------------+-------------------+-------------------+\n",
            "|  count|     16|                 16|                 16|                 16|\n",
            "|   mean|   null|0.30000000000000004| 1.5999999999999999| 1.9000000000000001|\n",
            "| stddev|   null| 0.1414213562373095|0.15916448515084428|0.24221202832779934|\n",
            "|    min| setosa|                0.1|                1.4|                1.5|\n",
            "|    max| setosa|                0.6|                1.9|                2.3|\n",
            "+-------+-------+-------------------+-------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now select records based on our conditions. Records that don't fulfill the conditions will be discarded. This binary behavior (\"keep or not\") only gets us so far. What if we want to transform the data in such a way that all records fulfilling a certain condition get mapped to 1 and all others to zero ? "
      ],
      "metadata": {
        "id": "EjoYUUQeHLK9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5. Conclusion\n",
        "\n",
        "You learned how to perform some basic transformations of the table, but maybe you also want to apply more complex functions to the DataFrame's rows or columns, such as summary statistics. In the next chapter, we are going to take a look at advanced transformations.\n"
      ],
      "metadata": {
        "id": "RcEXL7sKmO2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Functions and  Advanced Transformations\n",
        "Advanced transformations are where PySpark really shines, enabling us to execute very complex queries using simple syntax to extract valuable insights from our data. In this chapter we will see the power of methods such as `.groupBy()`, `.join()` especially in combination with more complex functions that are provided by the `functions` module. "
      ],
      "metadata": {
        "id": "xOWsFQy4cDM9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. Intro to Spark functions ?\n",
        "In general, it is possible to use functions from other libraries such as `numpy` on Spark `DataFrame` objects, however this defeats the purpose of Spark which is its ability to optimize the performance of transformation pipelines due to lazy execution. \n",
        "\n",
        "This is why `functions` exists, which provides the user with a copious amount of functions for all kinds of purposes.\n",
        "\n",
        "Suppose we want to map all record in the iris dataset whose petal width is above, let's say 0.5 to one and all others to zero. Although you wouldn't think of such an if-else statement as a function at first in Python in PySpark it is implemented in such a way through the function `.when()`.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ef25TfNJkhfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "(df1.select(\"species\",\n",
        "            F.when(df1[\"petalLength\"] > 0.5 ,1)\n",
        "             .otherwise(0).alias(\"pL above 0.5\"))\n",
        "    .show(5))"
      ],
      "metadata": {
        "id": "gxUxGZYJbnFc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f034032-529f-474b-cd68-9703cb1c8b7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------+\n",
            "|species|pL above 0.5|\n",
            "+-------+------------+\n",
            "| setosa|           1|\n",
            "| setosa|           1|\n",
            "| setosa|           1|\n",
            "| setosa|           1|\n",
            "| setosa|           1|\n",
            "+-------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, 0.5 is a pretty arbitrary value. Something like the mean or median would likely be more interesting. You might be tempted to write something like `df1[\"petalLength\"] > F.mean(\"petalLength\")` as the first argument of `F.when()` in line `7` above. This however will get you into \"*Teufels Küche*\" i.e. one hell of a mess.\n",
        "\n",
        "The problem is the following functions such as `F.mean()` aggregate the values of a column into one value, but in our case we want to compare each value of the \"petalLength\" column with the mean of the column.\n",
        "\n",
        "In order to deal with such dimensional mismatches we can use *window functions*, the way they are combined with `F.mean()` might look a bit unintuitive but what they do is basically return the mean, or the result of any other aggregation function, in form of a column containing as many rows as desired. In our case, we want to have as many rows as there are in the `df1.petalWidth`. Maybe a look at the graphic before the code will help you to get an even better understanding of what's going on.\n"
      ],
      "metadata": {
        "id": "fa-pSqhY2p9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "\n",
        "w = Window.rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
        "\n",
        "(df1.select(\"species\",\n",
        "            F.when(df1[\"petalLength\"] > F.mean(\"petalLength\").over(w) ,1)\n",
        "             .otherwise(0).alias(\"pL above mean\"))\n",
        "            .sample(fraction=0.1).show(5))"
      ],
      "metadata": {
        "id": "gKXcTbrc7klC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96525d6f-453f-4310-ed80-c56578ff23d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+\n",
            "|   species|pL above mean|\n",
            "+----------+-------------+\n",
            "|    setosa|            0|\n",
            "|    setosa|            0|\n",
            "|    setosa|            0|\n",
            "|    setosa|            0|\n",
            "|versicolor|            1|\n",
            "+----------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now what happens if the function we want to apply simply doesn't exist? After all, the `functions` module contains only a finite amount of functions and chaining and combining them will only get us so far. This is where *User Defined Functions (UDFs)* come into play. Buckle up!"
      ],
      "metadata": {
        "id": "gt2KgBAFarNr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2. User Defined Functions\n",
        "\n",
        "By allowing us to define their own functions, Spark gives us a lot of flexibility. After defining a Python function, we can register it in our Spark session via `spark.udf.register()`. It can then be used in all Spark pipelines just as functions from the `functions` module would be used. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bmSUzPKzhZbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def firstUpper(s: str) -> str:\n",
        "    s  = s[0].upper() + s[1:]\n",
        "    return s\n",
        "\n",
        "firstUpper_UDF = F.udf(firstUpper, \"STRING\")\n",
        "# Apply function to our DataFrame contraining the Iris data.\n",
        "df1.select(firstUpper_UDF(\"species\").alias(\"Species\")).show(2)"
      ],
      "metadata": {
        "id": "sZ3Kyft-iQ7Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb30d8e8-bc69-4b1d-e3dd-37427955db87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|Species|\n",
            "+-------+\n",
            "| Setosa|\n",
            "| Setosa|\n",
            "+-------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `firstUpper` we just wrote is quite trivial. From a code style perspective, it would be appropriate to substitute it via an anonymous function. Anonymous functions in Python can be created via the `lambda` keyword. \n"
      ],
      "metadata": {
        "id": "t2c56EaZ0lV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "firstUpper_UDF = F.udf(lambda s: s[0].upper() + s[1:], \"STRING\")\n",
        "# Apply function to our DataFrame contraining the Iris data.\n",
        "df1.select(firstUpper_UDF(\"species\").alias(\"Species\")).show(2)"
      ],
      "metadata": {
        "id": "O9KauDto0jH_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f4d5cd6-5a02-41e8-f441-f229662e2c10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|Species|\n",
            "+-------+\n",
            "| Setosa|\n",
            "| Setosa|\n",
            "+-------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you know, there is no free lunch. The flexibility that comes with UDF's come with a performance trade-off, since UDF's are a black-box to PySpark it can't use the optimization engines used for optimizing the execution of functions from the `functions` module. For this reason, you should use the functions from the `functions` module whenever possible.\n",
        "\n",
        "Sometimes we don't want to apply functions to all the entries in a column, but rather to a subset. An important way to generate such subsets is the `groupBy()` transformation, which we will see in action up next.\n"
      ],
      "metadata": {
        "id": "EYFQUfi817ki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3. Grouping Values by Attribute.\n",
        "What if we want to know how many plants belong to each species in our data set ? `.describe()` from section *1.1* didn't give us that information. In section *3.1* we've seen how to get the mean of the column of a data set, specifically the mean petalWidth of the iris flower. What if we want to get the specific mean of a certain species of iris flower ?\n",
        "\n",
        "This is where the `.groupBy()` transformation comes into play. It assigns entries of a data set to groups based on the values of those entries in a specific column. Afterwards, so-called *aggregation* functions can be applied to each group which map the group to a single object, usually a numerical value such as the mean.\n",
        "\n",
        "Figuring out how many plants belong to each species is an example of aggregation based on the same column by which the values are grouped.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KWGmm_0f8zVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(df1.select(\"species\")    # Consider the column \"species\" [string].\n",
        "    .groupBy(\"species\")   # Identical entries are assigned to the same group.\n",
        "    .count()              # For each group: count the number of entries belonging to it.\n",
        "    .orderBy(\"count\")     \n",
        "    .show(n=3))"
      ],
      "metadata": {
        "id": "HkeYICpksKbZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc139805-3da4-46f6-89f6-de66bdd3e2dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "|   species|count|\n",
            "+----------+-----+\n",
            "| virginica|   50|\n",
            "|versicolor|   50|\n",
            "|    setosa|   50|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting the mean petal width for each species could be done  similarly to the code above. In the case of the mean, however, we might also want to round our results and give them a concise name. Maybe we are also interested in knowing the maximum petal width or other summary statistics of each species petal width.\n",
        "\n",
        "In general, the `.agg()` transformation takes a list or dictionary of aggregation functions as input and applies them to the groups created by `.groupBy()`. A typical workflow could look like this:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sDZA70U33JUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(df1.select(\"petalWidth\", \"species\")            # Consider the two columns \"petalWidth\" [double] and \"species\" [string].\n",
        "    .where(F.col(\"petalWidth\").isNotNull())     # Only consider entries where the petalWidth attribute is not Null.\n",
        "    .groupBy(\"species\")                         # Group the entries in the petalWidth column by their species.\n",
        "    .agg(F.round(F.mean(\"petalWidth\"),2)        # Calculate the mean petalWidth of each species rounded to two digits.\n",
        "          .alias(\"mean_pW\"),\n",
        "         F.max(\"petalWidth\"))                   # Name resulting column containing the means \"mean_pW\".\n",
        "    .orderBy(\"mean_pW\", ascending=False)        \n",
        "    .show(n=3))"
      ],
      "metadata": {
        "id": "S3MfZN08_i9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c071d15-68b1-4d83-f638-f9549ebb5826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+---------------+\n",
            "|   species|mean_pW|max(petalWidth)|\n",
            "+----------+-------+---------------+\n",
            "| virginica|   2.03|            2.5|\n",
            "|versicolor|   1.33|            1.8|\n",
            "|    setosa|   0.25|            0.6|\n",
            "+----------+-------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've been introduced to `.count()` and `.groupBy()` we can even specify our analysis of the petal Length from *3.1*. We can now find out the number of plants from each species whose petal length is above the mean petal length for the respective species. This section of code is probably the most complicated one in the tutorial, but pondering over it will give your understanding of the material a huge boost, so take a good look!"
      ],
      "metadata": {
        "id": "bsdjNnSE9B3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = Window.partitionBy(\"species\")                                               # Apply window species-wise.\n",
        "\n",
        "df = (df1.select(\"species\",                                                         \n",
        "                 F.when(df1.petalLength > F.mean(\"petalLength\").over(w), 1)          \n",
        "                  .otherwise(0).alias(\"pL above species mean\"))                      \n",
        "         .groupBy(\"species\")\n",
        "         .agg((F.sum(\"pL above species mean\") / F.count(\"species\"))                       \n",
        "                .alias(\"pL larger than species mean (%)\")))\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "id": "Vrv9GbaX-SdH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "175676d2-8411-474e-93f3-fb54d194f84a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------------------------+\n",
            "|   species|pL larger than species mean (%)|\n",
            "+----------+-------------------------------+\n",
            "|    setosa|                           0.52|\n",
            "|versicolor|                           0.54|\n",
            "| virginica|                            0.5|\n",
            "+----------+-------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All that was a bit tougher than the concepts in the second  chapter, after all we are dealing with *advanced* transformations. The next and final  concept we will learn about is the `.join()` that some of you might know from working with SQL."
      ],
      "metadata": {
        "id": "frsUYiambPoG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4. Joining DataFrames.\n",
        "Suppose we have a set of *entities* (flowers in our concrete example) whose *attributes* are spread over multiple datasets. This means that one dataset might contain the petal width and another the petal length. A logical thing to do would be to combine or *join* the datasets (which are our DataFrames) into one big dataset containing all attributes associated with an entity.\n",
        "\n",
        "Before joining two DataFrames in PySpark we have to decide on two things: Based on which attributes to join the two DataFrames and how to join them. In simple cases both DataFrames contain an ID column which can easily be used to join  records from both DataFrames.\n",
        "\n",
        "The question \"how to join\" arises when the DataFrames don't contain the same records. We could decide to only consider records corresponding to IDs that are contained in both columns. This would lead to loss of information however, since we would have to discard records that contain non-Null values. Alternatively we could fill the columns from the DataFrame in which the ID isn't contained with Null-values. \n",
        "\n",
        "The approaches we have described here can be selected in `.join()` by setting `how=\"inner\"` or `how=\"full_outer\"` respectively. There are other types of joins such as *left joins* and *right joins* as well as others which make the `.join()` transformation a potent tool for data combination. The `on` argument indicates which common column of both DataFrames to use as criterium for determining which records to combine.\n",
        "\n",
        "The idea behind joins can be neatly visualized via Venn-diagrams. Take a look at the graphic before running the code cell below.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TAZPXRE6csuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_left = df1.select(\"petalWidth\", \"species\")\n",
        "df_right = df1.select(\"petalLength\", \"species\")\n",
        "\n",
        "df_left.join(df_right, on=\"species\", how=\"inner\").show(5)"
      ],
      "metadata": {
        "id": "Ulo2LE4GlheF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84370310-91dd-4fa5-8cc5-a069299ae73a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-----------+\n",
            "|species|petalWidth|petalLength|\n",
            "+-------+----------+-----------+\n",
            "| setosa|       0.2|        1.4|\n",
            "| setosa|       0.2|        1.5|\n",
            "| setosa|       0.2|        1.4|\n",
            "| setosa|       0.2|        1.6|\n",
            "| setosa|       0.2|        1.4|\n",
            "+-------+----------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5. Conclusion.\n",
        "\n",
        "Nice! You went through some pretty serious stuff! All in all you learned how to extract very specific information from data via functions, windows and `groupBy()` transformations as well as flexibly merging DataFrames via `.join()`. In the final chapter you will see how to perform the final step of an ETL workflow and that is loading the extracted and transformed data from the staging are into a data warehouse. "
      ],
      "metadata": {
        "id": "KsyEfzy2xnNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. The Loading Step.\n",
        "\n",
        "Now that we have gained an in-depth look on how to do transformations, we can come to the final step of ETL which is loading our data from the staging are (the cells where we performed transformations) to our data warehouse. We will store the data in form of a Parquet file and an SQL table to show the various possibilities the user has for performing the loading step in PySpark."
      ],
      "metadata": {
        "id": "f19m3l4TyN5c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1. What's a parquet file ?\n",
        "According to databricks, a company founded by the developers of Spark\n",
        "> Apache Parquet is an open source, column-oriented data file format designed for efficient data storage and retrieval. It provides efficient data compression and encoding schemes with enhanced performance to handle complex data in bulk.\n",
        "\n",
        "It can be used to store data of all kinds of formats such as structured data tables, images, videos and documents. It uses a range of sophisticated techniques to optimize aspects such as storage space and query speed. Parquet files are able to only read the needed columns for a given query, therefore greatly minimizing the IO for instance.\n",
        "\n",
        "The advantages of using Parquet files are especially clear when we look at the monetary cost of working with them instead of .csv files. According to databricks costs for storing data in an AWS S3 bucket and running querries on it can be 99.7% less costly using .parquet instead of .csv files."
      ],
      "metadata": {
        "id": "rg8LlpPuhYtY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2. Load Data into a Parquet file.\n",
        "Loading data from PySpark into a Paqruet file is very straight forward, we just have to use the `.write` transformation, which turns the DataFrame into a *DataFrameWriter*. We then apply methods to specify things such as the format of the file and it's location. Note that colab automatically saves files to the *\\content* folder if no other path is given. Feel free to click on the folder icon left of this code cell to get a better idea of how Parquet files are stored. The `.mode()` method is used to handle the case where the data already exists in the location specified in `.save()`."
      ],
      "metadata": {
        "id": "l0WM8q6gugz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify what .write does.\n",
        "print(type(df.write))\n",
        "print(type(df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxo59a_HXo_w",
        "outputId": "288d9ff3-506e-4f10-9e24-5195d96151f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.sql.readwriter.DataFrameWriter'>\n",
            "<class 'pyspark.sql.dataframe.DataFrame'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumnRenamed(\"pL larger than species mean (%)\",'pL_larger_than_species_mean') #Parquet has special naming conventions for tables.\n",
        "\n",
        "df.write.format(\"parquet\").mode(\"overwrite\").save(\"df_parquet\")"
      ],
      "metadata": {
        "id": "eVjVbGdTzdUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assume we want to query only entries of the file with a certain attribute such as the species being \"setosa\". Usually we would specify this restriction during our query. With parquet files however, we can include such a restriction during the reading of the file from our data warehouse if we use the `.partitionBy()` during the load step. This method essentially emulates the `.groupBy()` transformation by assigning entries with different values for the chosen partition attributes to different subsections of the parquet file. During reading we can then access subsections of the files potentially saving us a lot of loading time."
      ],
      "metadata": {
        "id": "S19ILaSMiveL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.format(\"parquet\").partitionBy(\"species\").mode(\"overwrite\").save(\"df_parquet\")"
      ],
      "metadata": {
        "id": "DhmsJbJ_QBjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a look again at the content folder in the directory of this colab notebook to see how the structure of the parquet file changed."
      ],
      "metadata": {
        "id": "TxuNPU24k-fk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3. Registering a Parquet file as a SQL table.\n",
        "If we want to we can also load the Parquet file into a SQL table on which we then can run SQL querries as usual. In order to do so we only have to replace the `.save()` method with the `.saveAsTable()` method."
      ],
      "metadata": {
        "id": "xln91Otpm97I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"df_parquet\")\n",
        "spark.sql('SELECT * from df_parquet').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFzlFsTSnmYY",
        "outputId": "f295ab6e-24c2-4488-d2b6-e87c3ae43e99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------------------------+\n",
            "|   species|pL_larger_than_species_mean|\n",
            "+----------+---------------------------+\n",
            "|    setosa|                       0.52|\n",
            "|versicolor|                       0.54|\n",
            "| virginica|                        0.5|\n",
            "+----------+---------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4. Loading to a SQLite table.\n",
        "Since this tutorial follows up on a SQLite tutorial it make sense to take a look at how to load our results into a SQLite table, since you could then use all the knowledge gained in the preceeding tutorial and apply it in combination with your newly gained PySpark skills. In order to perform the loading step we first have to transform the PySpark DataFrame to a pandas DataFrame, which is then stored in our SQLite table via the `.to_sql()` from pandas.\n"
      ],
      "metadata": {
        "id": "q9XlGGzbtI8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify change of type.\n",
        "print(type(df.toPandas()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-A7WruBksMk",
        "outputId": "4146e74f-0b41-47a4-cab3-fedc2777f37f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "connection = sqlite3.connect('result.sqlite')\n",
        "\n",
        "df.toPandas().to_sql('water_consumption', connection, if_exists='replace', index=True)\n",
        "\n",
        "print(connection.execute('SELECT * FROM water_consumption LIMIT 3;').fetchall())\n",
        "\n",
        "connection.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZwpXyhNjkdh",
        "outputId": "c9b9e8ef-90e5-4640-8d21-804c4849febf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 'setosa', 0.52), (1, 'versicolor', 0.54), (2, 'virginica', 0.5)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.4. Conclusion.\n",
        "You've seen now an example of how we can perform the loading step in an ETL pipeline. We've hereby come to the end of our tutorial. You should now be able to set up an ETL pipeline in PySpark yourself gathering interesting insights and managing the flow of data in an efficient way by using schemas for reading data and .parquet files to load it into a data warehouse. Feel free to take a look at our sources to dig deeper yourself, there is still a lot of valuable things to learn about Spark and we hope we sparked your curiosity for doing so."
      ],
      "metadata": {
        "id": "jK5_mNSM5EXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Sources."
      ],
      "metadata": {
        "id": "VXyx8tGT7ADt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Karau, H., Konwinski, A., Wendell, P., Zaharia, M. (2015). Learning Spark. O'Reilly. ISBN: 9781449358624 \n",
        "\n",
        "2. https://databricks.com/de/glossary/what-is-parquet\n",
        "\n",
        "4. https://spark.apache.org/docs/latest/\n",
        "\n",
        "5. https://sparkbyexamples.com/\n",
        "\n",
        "6. https://www.ibm.com/cloud/learn/etl\n"
      ],
      "metadata": {
        "id": "wm_UURYO7F8w"
      }
    }
  ]
}