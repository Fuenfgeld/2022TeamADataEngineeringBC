{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PySparkTutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "FjsR9GZemXN9",
        "FSLy8EIT9Qs1",
        "ClcImNjp1pRm",
        "-23HYBWb277h",
        "AvnZOVxV7K9t",
        "NKioCZzjlL-L",
        "o_9RPA4ZL7W9",
        "RtB5fpk0flQQ",
        "Ef25TfNJkhfs"
      ],
      "authorship_tag": "ABX9TyPtkvfrEtOvY56BciH6LB3E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fuenfgeld/2022TeamADataEngineeringBC/blob/main/PySparkTutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##0. Data Engineering Bootcamp\n",
        "In this tutorial you will be introduced to an aspect of Data Engineering called ETL. Together we will implement an ETL workflow with Apache Spark in Python. By the end of the tutorial you will be able to adapt such a workflow to your specific needs and the benefits of using Spark in doing so."
      ],
      "metadata": {
        "id": "-DZ60lbz97U4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 0.1 What is Data Engineering ?\n",
        "\n",
        "Data engineering is the practice of designing and building systems for collecting, storing, and analyzing data at scale. Data engineers work in a variety of settings to build systems that collect, manage, and convert raw data into usable information for data scientists and business analysts to interpret. Their ultimate goal is to make data accessible so that organizations can use it to evaluate and optimize their performance. This last sentence also sums up the difference between a data engineer and a data analyst, whereas the former manages the data resources the later exploits them to gain valuable insights.\n",
        "\n",
        "### 0.2 What is ETL ?\n",
        "\n",
        "According to IBM ETL, which stands for extract, transform and load, is a data integration process that combines data from multiple data sources into a single, consistent data store. It is closely linked with the concept of a *Data Warehouse* describes central repositories of integrated data from one or more disparate sources. \n",
        "\n",
        "#### Extraction\n",
        "During data extraction, raw data is copied or exported from source locations from a variety of data sources, which can be structured or unstructured such as SQL databases, json files or even web pages.\n",
        "#### Transformation\n",
        "The collected raw data then undergoes data processing. Here, the data is transformed and consolidated for its intended analytical use case. Steps taken during transformation are de-duplicating values, performing calculations, translations, or summarizations based on the raw data and changing the shape of the dataa via joining and grouping operation in order to match the schema of the target data warehouse. The environment in which the transformation step is performed is also called *staging area*.\n",
        "#### Loading\n",
        "In this last step, the transformed data is moved from the staging area into a target data warehouse. Typically, this involves an initial loading of all data, followed by periodic loading of incremental data changes \n",
        "\n",
        "### 0.3 What is Spark ?\n",
        "\n",
        "According to the official website\n",
        "\n",
        ">*Apache Spark is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters.*\n",
        "\n",
        "Now what does that mean in ? You can think of Spark as a programming library that allows you to outsource your data engineering workflow to a set of servers (cluster) which enables you to parallelize operations, enabling faster execution and the ability to work with amounts of data that couldn't be handled on a single computer (Big Data). \n",
        "\n",
        "Hence what Spark does is managing the interaction between your local  node (computer) and each node (server) of the cluster. Since Spark was originally written in Scala there is no direct way to access its functionality in Python. This is where *PySpark* comes into play. You can think of PySpark as a Python-based wrapper on top of the Scala API there are also similar wrappers for *R* and other programming languages, this is why the official website describes Spark as a *multi-language engine*. \n",
        "\n",
        "#### SparkSQL\n",
        "\n",
        "Although the *Resilient Distributed Dataset* (RDD) is the  fundamental data structure on which all higher-level data structures are constructed in Spark, this tutorial is going to focus on the *DataFrame* from the SparkSQL model which deals with structured data such as .json and .csv files. \n",
        "\n",
        "The DataFrame has two big advantages over the RDD. First it has  significant performance benefits over RDDs due to a powerful optimization engine and secondly important data science module such as *spark.streaming* an *spark.ml* work with DataFrames instead of RDDs.\n",
        "\n",
        "\n",
        "## Conclusion\n",
        "You learned what data engineering is,how an ETL workflow is structured and what role Spark plays in such a  contex. Now let's get started with coding stuff. \n",
        "\n",
        "The next three lines of code will make Pyspark and all the relevant data you need to finish this tutorial available in your Colab notebook.\n"
      ],
      "metadata": {
        "id": "tidhxKfZlyJz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0kMaqIMwKk-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8e38ddb-147d-4594-cc61-7118a901a132"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 34 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 50.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=7375a861ed24a80cfb638fa5e2d8016bb8c74ff2940fe3e8570b65cdc910452b\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -cq https://raw.githubusercontent.com/Fuenfgeld/2022TeamADataEngineeringBC/ca4b2ecc9e9ee242037d11c27edd4f4ad770e7ee/iris.json"
      ],
      "metadata": {
        "id": "AWFXHurvlAP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -cq https://raw.githubusercontent.com/Fuenfgeld/2022TeamADataEngineeringBC/main/iris2.json"
      ],
      "metadata": {
        "id": "UwhFqG5OpkrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Getting Started \n",
        "\n",
        "Let's get started building our first Spark application. At the core of ou Spark application is the *SparkSession* object which acts as a point of entry to interact with underlying Spark\n",
        "functionalities.\n",
        "\n",
        "Usually Spark would delegate the computation jobs to the so called *executors* (CPUs in each node of the cluster) through the so called *driver*. Since this notebook isn't connected to a cluster the jobs will be performed locally thoug. The concept however is still the same just be aware that we aren't exploiting Sparks full capabilities here for practical reasons (Clusters are not trivial to deal with).\n"
      ],
      "metadata": {
        "id": "FjsR9GZemXN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "WQMsUJrIM3-I"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our Spark session is initialized we can load in some data to work with. Spark can handle data from all kinds of sources such as .json files, .csv files and even access data from AWS or Azure via dedicated interfaces."
      ],
      "metadata": {
        "id": "ExNrRpo4-HYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = spark.read.option(\"multiline\",True).json('iris.json')\n",
        "print(f\"Object Type: {type(df1)}\\n\")\n",
        "print(\"Column Info:\")\n",
        "df1.printSchema()\n",
        "print(\"Summary Statistics of columns:\")\n",
        "df1.describe().show()\n",
        "print(\"Overview Dataframe:\")\n",
        "df1.show(10)"
      ],
      "metadata": {
        "id": "Kc5OYn6VNa9S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a254b271-310c-4998-b29c-29bf87b4e083"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object Type: <class 'pyspark.sql.dataframe.DataFrame'>\n",
            "\n",
            "Column Info:\n",
            "root\n",
            " |-- petalLength: double (nullable = true)\n",
            " |-- petalWidth: double (nullable = true)\n",
            " |-- sepalLength: double (nullable = true)\n",
            " |-- sepalWidth: double (nullable = true)\n",
            " |-- species: string (nullable = true)\n",
            "\n",
            "Summary Statistics of columns:\n",
            "+-------+------------------+------------------+------------------+-------------------+---------+\n",
            "|summary|       petalLength|        petalWidth|       sepalLength|         sepalWidth|  species|\n",
            "+-------+------------------+------------------+------------------+-------------------+---------+\n",
            "|  count|               150|               150|               150|                150|      150|\n",
            "|   mean|3.7580000000000027| 1.199333333333334| 5.843333333333335|  3.057333333333334|     null|\n",
            "| stddev|1.7652982332594662|0.7622376689603467|0.8280661279778637|0.43586628493669793|     null|\n",
            "|    min|               1.0|               0.1|               4.3|                2.0|   setosa|\n",
            "|    max|               6.9|               2.5|               7.9|                4.4|virginica|\n",
            "+-------+------------------+------------------+------------------+-------------------+---------+\n",
            "\n",
            "Overview Dataframe:\n",
            "+-----------+----------+-----------+----------+-------+\n",
            "|petalLength|petalWidth|sepalLength|sepalWidth|species|\n",
            "+-----------+----------+-----------+----------+-------+\n",
            "|        1.4|       0.2|        5.1|       3.5| setosa|\n",
            "|        1.4|       0.2|        4.9|       3.0| setosa|\n",
            "|        1.3|       0.2|        4.7|       3.2| setosa|\n",
            "|        1.5|       0.2|        4.6|       3.1| setosa|\n",
            "|        1.4|       0.2|        5.0|       3.6| setosa|\n",
            "|        1.7|       0.4|        5.4|       3.9| setosa|\n",
            "|        1.4|       0.3|        4.6|       3.4| setosa|\n",
            "|        1.5|       0.2|        5.0|       3.4| setosa|\n",
            "|        1.4|       0.2|        4.4|       2.9| setosa|\n",
            "|        1.5|       0.1|        4.9|       3.1| setosa|\n",
            "+-----------+----------+-----------+----------+-------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might have wondered why we called `.show()` behind `.describe()` in line 6, especially if you are familiar with *Pandas*. The reason is the so called *Lazy Execution* where code is only executed  once so called *actions* are called. The next chapter will begin introducing these concepts in more depth. We will also get to know *transformations*, code that changes the structure and entries of DataFrames."
      ],
      "metadata": {
        "id": "s8_T1h5sC_Sk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Actions and Basic Transformations\n",
        "\n",
        "Spark operations on distributed data can be classified into two types: *transformations* and *actions*. Transformations, as the name suggests, transform a Spark DataFrame into a new DataFrame without altering the original data, giving it the property of *immutability*. \n",
        "\n",
        "\n",
        "All transformations are evaluated lazily. That is, their results are not computed immediately, but they are recorded or remembered as a *lineage*. A recorded lineage allows Spark, at a later time in its execution plan, to rearrange certain transformations or optimize them into stages for more efficient execution. \n",
        "\n",
        "Because Spark records each transformation in its lineage and the DataFrames are immutable between transformations, it can reproduce its original state by simply replaying the recorded lineage, giving it resiliency in the event of failures.\n",
        "\n",
        "Now let's set up some simple Data Engineering workflows in Py Spark using actions and transformations"
      ],
      "metadata": {
        "id": "92lmS82aSNDx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Accessing Rows\n",
        "###2.1. Accessing Rows\n",
        "\n",
        "Since Spark was concieved to work with distributed data there is no simple way to access rows at will.\n",
        "\n",
        "If you want to do so anyways you have the possibility to pull the data onto your local node.\n",
        "\n",
        "The action `.collect()` collects the distributed data to the driver side as local data in Python. Note that this can throw an out-of-memory error when the dataset is too large to fit in the driver side because it collects all the data from executors to the driver side."
      ],
      "metadata": {
        "id": "FSLy8EIT9Qs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns list of Row objects\n",
        "local_df1 = df1.collect()\n",
        "print(f\"Type of entries: {type(local_df1[0])}\\n\")\n",
        "print(f\"Entries: {local_df1[:5]}\")"
      ],
      "metadata": {
        "id": "wqjHubRZSVJg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25c3bad6-e103-45df-9a8e-207ebf14d0bc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of entries: <class 'pyspark.sql.types.Row'>\n",
            "\n",
            "Entries: [Row(petalLength=1.4, petalWidth=0.2, sepalLength=5.1, sepalWidth=3.5, species='setosa'), Row(petalLength=1.4, petalWidth=0.2, sepalLength=4.9, sepalWidth=3.0, species='setosa'), Row(petalLength=1.3, petalWidth=0.2, sepalLength=4.7, sepalWidth=3.2, species='setosa'), Row(petalLength=1.5, petalWidth=0.2, sepalLength=4.6, sepalWidth=3.1, species='setosa'), Row(petalLength=1.4, petalWidth=0.2, sepalLength=5.0, sepalWidth=3.6, species='setosa')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Accessing Columns\n",
        "\n",
        "Accessing columns doesn't come with the difficulties associated with handling rows. If we want to get specific columns we can simply do so through the `.select()` method. "
      ],
      "metadata": {
        "id": "ClcImNjp1pRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.select(\"petalLength\").show(5)"
      ],
      "metadata": {
        "id": "YjXEYeYSPu6J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c708948-d698-4237-ef07-aa140c231ecf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|petalLength|\n",
            "+-----------+\n",
            "|        1.4|\n",
            "|        1.5|\n",
            "|        6.0|\n",
            "|        4.9|\n",
            "|        4.4|\n",
            "+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is also possible to choose mutiple columns. Notice that we can adress our columns with `DataFrame.NameOfColumn` instead of `\"NameOfColumn\"`."
      ],
      "metadata": {
        "id": "Z0jSHnkiYZ6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "petalLength = df1.petalLength\n",
        "petalWidth = df1.petalWidth\n",
        "df1.select(petalLength, petalWidth).show(5)"
      ],
      "metadata": {
        "id": "PxgmXX92Yi-H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f99c9043-775a-46a8-e60a-0cf230c5b0d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+\n",
            "|petalLength|petalWidth|\n",
            "+-----------+----------+\n",
            "|        1.4|       0.2|\n",
            "|        1.4|       0.2|\n",
            "|        1.3|       0.2|\n",
            "|        1.5|       0.2|\n",
            "|        1.4|       0.2|\n",
            "+-----------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. Concatenating DataFrames"
      ],
      "metadata": {
        "id": "-23HYBWb277h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we have a dataset that is split into multiple DataFrames. Wouldn't it be practical to combine them into one table ? `pyspark` provides such a funcionality via the `.union()` method."
      ],
      "metadata": {
        "id": "yOmxStmxdPYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = spark.read.json('iris2.json')\n",
        "df2.show()\n",
        "df1.union(df2)"
      ],
      "metadata": {
        "id": "Ei_V2RL5ZhSj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b162e299-c5c5-493c-9c8b-6b71a90bca09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+-----------+----------+---------+\n",
            "|petalLength|petalWidth|sepalLength|sepalWidth|  species|\n",
            "+-----------+----------+-----------+----------+---------+\n",
            "|        5.1|       1.8|        5.9|       3.0|virginica|\n",
            "+-----------+----------+-----------+----------+---------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[petalLength: double, petalWidth: double, sepalLength: double, sepalWidth: double, species: string]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Adding Columns\n",
        "\n",
        "In case we want to add columns we can do so via the `.withColumn()` method. Note that we have to specify the name of the column which is in this case `petalSum`. Usually the new column is a function of one or more of the old columns. "
      ],
      "metadata": {
        "id": "AvnZOVxV7K9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_extraCol = df1.withColumn('newColumn', df1.petalWidth + df1.petalLength)\n",
        "df_extraCol.show(5)"
      ],
      "metadata": {
        "id": "7XRQiJVSij7m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ec38faa-c950-46fe-f594-666720f5f79f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+-----------+----------+-------+------------------+\n",
            "|petalLength|petalWidth|sepalLength|sepalWidth|species|         newColumn|\n",
            "+-----------+----------+-----------+----------+-------+------------------+\n",
            "|        1.4|       0.2|        5.1|       3.5| setosa|1.5999999999999999|\n",
            "|        1.4|       0.2|        4.9|       3.0| setosa|1.5999999999999999|\n",
            "|        1.3|       0.2|        4.7|       3.2| setosa|               1.5|\n",
            "|        1.5|       0.2|        4.6|       3.1| setosa|               1.7|\n",
            "|        1.4|       0.2|        5.0|       3.6| setosa|1.5999999999999999|\n",
            "+-----------+----------+-----------+----------+-------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The name `'newColumn'` isn't really informative. It's therefore hard for the user to deduce that is it the sum of `'petalWidth'` and `'petalLength'`. So why not rename it to something more indicative ? We can do this via the `.withColumnRenamed()` method."
      ],
      "metadata": {
        "id": "Pf1MnUAcI7M5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_extraCol = df_extraCol.withColumnRenamed('newColumn','petalSum')\n",
        "df_extraCol.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHeB_op4JtMF",
        "outputId": "4c20d380-39b1-429d-a029-35befff12045"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+-----------+----------+-------+------------------+\n",
            "|petalLength|petalWidth|sepalLength|sepalWidth|species|          petalSum|\n",
            "+-----------+----------+-----------+----------+-------+------------------+\n",
            "|        1.4|       0.2|        5.1|       3.5| setosa|1.5999999999999999|\n",
            "|        1.4|       0.2|        4.9|       3.0| setosa|1.5999999999999999|\n",
            "|        1.3|       0.2|        4.7|       3.2| setosa|               1.5|\n",
            "|        1.5|       0.2|        4.6|       3.1| setosa|               1.7|\n",
            "|        1.4|       0.2|        5.0|       3.6| setosa|1.5999999999999999|\n",
            "+-----------+----------+-----------+----------+-------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5. Removing Columns\n",
        "\n",
        "In order to get rid of our new column `.drop()` can be used. In contrast to `.select()`, this method removes the specified column completely instead of returning it as slice ot the table.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NKioCZzjlL-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df_extraCol.drop(df_extraCol.petalSum)\n",
        "df1.show(5)"
      ],
      "metadata": {
        "id": "Xp8ScsA-ixVF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54344dc7-9a6a-410f-f9c5-bfbb8fd1da5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+-----------+----------+-------+\n",
            "|petalLength|petalWidth|sepalLength|sepalWidth|species|\n",
            "+-----------+----------+-----------+----------+-------+\n",
            "|        1.4|       0.2|        5.1|       3.5| setosa|\n",
            "|        1.4|       0.2|        4.9|       3.0| setosa|\n",
            "|        1.3|       0.2|        4.7|       3.2| setosa|\n",
            "|        1.5|       0.2|        4.6|       3.1| setosa|\n",
            "|        1.4|       0.2|        5.0|       3.6| setosa|\n",
            "+-----------+----------+-----------+----------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6. Basic Data Cleaning\n",
        "\n",
        "Just as in the hospital, hygiene is of great importance to working with data, sometimes rows contain entries that make dealing with our data more difficult or lower its quality (information pollution). Two examples come to mind: Duplicate entries could bias introduce into our data which negatively impacts the performance of a lot of machine learning algorithms.\n",
        "\n",
        "The second example would be null entries which might render some rows useless due to the fact that most algorithms generally can't handle such entries. Luckily PySpark provides us with two methods `.dropna()` and `.dropDuplicates()` to get rid of such problematic rows.\n",
        "\n"
      ],
      "metadata": {
        "id": "o_9RPA4ZL7W9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df1.dropna()"
      ],
      "metadata": {
        "id": "22OiPKXUTVVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df1.dropDuplicates() "
      ],
      "metadata": {
        "id": "lNGMK9rwR4rY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although our dataframe is now free of unwanted entries we might still want to put further restrictions on the data we want to keep. "
      ],
      "metadata": {
        "id": "Em-5JUVf82Kg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.7. Conditional Selection of Rows.\n",
        "\n",
        "In 2.1. we explained that directly accessing rows of a DataFrame comes with some caveats, it is however possible to indirectly access rows without pulling all the data onto your local node. This is done via conditional selection where we select rows based on user given conditions via the `.filter()` method. This means however that we don't know which rows we will obtain in the end, hence why we speak of indirect access.\n",
        "\n",
        "Let's say we want to get only the flowers of type `\"virginica\"` we then have to write the following:"
      ],
      "metadata": {
        "id": "RtB5fpk0flQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_virginica = df1.filter(df1.species == \"virginica\")\n",
        "df_virginica.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XN6A0o-ziDhm",
        "outputId": "cea133a3-525a-4be7-fb4b-b09ac8a387e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+-----------+----------+---------+\n",
            "|petalLength|petalWidth|sepalLength|sepalWidth|  species|\n",
            "+-----------+----------+-----------+----------+---------+\n",
            "|        6.0|       1.8|        7.2|       3.2|virginica|\n",
            "|        5.6|       2.1|        6.4|       2.8|virginica|\n",
            "|        5.1|       2.3|        6.9|       3.1|virginica|\n",
            "|        6.1|       2.5|        7.2|       3.6|virginica|\n",
            "|        5.7|       2.3|        6.9|       3.2|virginica|\n",
            "+-----------+----------+-----------+----------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.8 Alter data based using Lambda\n",
        "\n",
        "Using the `map` function columns and the full structure can be altered using Lambdas."
      ],
      "metadata": {
        "id": "-KQh0yhFdgTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import types, functions\n",
        "\n",
        "data = [\n",
        "        ('Max', 'Mustermann', 'm', '10', '1954', '2020'),\n",
        "        ('Erika', 'Mustermann', 'w', '12', '1994', None)\n",
        "        ]\n",
        "schema = ['firstname', 'lastname', 'gender', 'salary', 'birthyear', 'deathyear']\n",
        "\n",
        "frame = spark.createDataFrame(data = data, schema = schema)\n",
        "frame.show()\n",
        "\n",
        "# To cast or alter a column, just override it\n",
        "parsed = frame.withColumn('salary', functions.col('salary').cast(types.IntegerType()))\n",
        "\n",
        "# Single column transformations\n",
        "doubled = parsed.withColumn('salary', functions.col('salary') * 2)\n",
        "\n",
        "# Conditional replacements are possible using functions\n",
        "replaceNullValue = doubled.withColumn(\n",
        "    'deathyear', \n",
        "    functions.when(functions.col('deathyear').isNull(), '2022')\n",
        "    .otherwise(functions.col('deathyear'))\n",
        ")\n",
        "\n",
        "# Spark has its own mapping language which can be used in withColumn\n",
        "withAge = replaceNullValue.withColumn('age', functions.col('deathyear') - functions.col('birthyear'))\n",
        "\n",
        "# To replace a value with a value in a dictionary, you replace the value on the whole\n",
        "# dataset and restrict the changes to the columns in which it should be replaced\n",
        "genders = { 'm': 'male', 'w': 'female' }\n",
        "withGender = withAge.replace(genders, subset='gender')\n",
        "withGender.show()\n",
        "\n",
        "# Lambdas can also be used. They are slower but more powerful and can alter the schema\n",
        "converted = (withGender.rdd\n",
        "  .map(lambda row: (row[0] + ' ' + row[1], row[2], row[3], row[6]))\n",
        "  .toDF(['name', 'gender', 'salary', 'age'])\n",
        ")\n",
        "converted.show()\n",
        "\n",
        "# The transformations can also be written functionally\n",
        "functional = (spark.createDataFrame(data = data, schema = schema)\n",
        "    .withColumn('salary', functions.col('salary').cast(types.IntegerType()))\n",
        "    .withColumn('salary', functions.col('salary') * 2)\n",
        "    .withColumn(\n",
        "        'deathyear', \n",
        "        functions.when(functions.col('deathyear').isNull(), '2022')\n",
        "        .otherwise(functions.col('deathyear'))\n",
        "    )\n",
        "    .withColumn('age', functions.col('deathyear') - functions.col('birthyear'))\n",
        "    .replace(genders, subset='gender')\n",
        "    .rdd\n",
        "    .map(lambda row: (row[0] + ' ' + row[1], row[2], row[3], row[6]))\n",
        "    .toDF(['name', 'gender', 'salary', 'age'])\n",
        ")\n",
        "functional.show()"
      ],
      "metadata": {
        "id": "y5tbJpJ2d1na",
        "outputId": "4dc9ef68-662c-4c5e-a3d2-5e5d84d9a3e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+------+------+---------+---------+\n",
            "|firstname|  lastname|gender|salary|birthyear|deathyear|\n",
            "+---------+----------+------+------+---------+---------+\n",
            "|      Max|Mustermann|     m|    10|     1954|     2020|\n",
            "|    Erika|Mustermann|     w|    12|     1994|     null|\n",
            "+---------+----------+------+------+---------+---------+\n",
            "\n",
            "+---------+----------+------+------+---------+---------+----+\n",
            "|firstname|  lastname|gender|salary|birthyear|deathyear| age|\n",
            "+---------+----------+------+------+---------+---------+----+\n",
            "|      Max|Mustermann|  male|    20|     1954|     2020|66.0|\n",
            "|    Erika|Mustermann|female|    24|     1994|     2022|28.0|\n",
            "+---------+----------+------+------+---------+---------+----+\n",
            "\n",
            "+----------------+------+------+----+\n",
            "|            name|gender|salary| age|\n",
            "+----------------+------+------+----+\n",
            "|  Max Mustermann|  male|    20|66.0|\n",
            "|Erika Mustermann|female|    24|28.0|\n",
            "+----------------+------+------+----+\n",
            "\n",
            "+----------------+------+------+----+\n",
            "|            name|gender|salary| age|\n",
            "+----------------+------+------+----+\n",
            "|  Max Mustermann|  male|    20|66.0|\n",
            "|Erika Mustermann|female|    24|28.0|\n",
            "+----------------+------+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Join data based on key"
      ],
      "metadata": {
        "id": "5coT1v-8vFNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "people = spark.createDataFrame(data = [( 'Max', 1 ), ( 'Erika', 0 )],\n",
        "                               schema = ['name', 'cityId'])\n",
        "people.show()\n",
        "cities = spark.createDataFrame(data=[(0, 'Mannheim'), (1, 'Frankfurt')],\n",
        "                               schema=['cityId', 'city'])\n",
        "cities.show()\n",
        "\n",
        "combined = people.join(cities, ['cityId'], \"inner\").drop('cityId')\n",
        "combined.show()"
      ],
      "metadata": {
        "id": "FqxzQ9EmvEoX",
        "outputId": "1f8b7631-e460-4b1a-a6ff-7a2a249b608b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+\n",
            "| name|cityId|\n",
            "+-----+------+\n",
            "|  Max|     1|\n",
            "|Erika|     0|\n",
            "+-----+------+\n",
            "\n",
            "+------+---------+\n",
            "|cityId|     city|\n",
            "+------+---------+\n",
            "|     0| Mannheim|\n",
            "|     1|Frankfurt|\n",
            "+------+---------+\n",
            "\n",
            "+-----+---------+\n",
            "| name|     city|\n",
            "+-----+---------+\n",
            "|Erika| Mannheim|\n",
            "|  Max|Frankfurt|\n",
            "+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.8. Conclusion\n",
        "\n",
        "You learned how to perform some basic transformations of the table, but maybe you also want to apply more complex functions to the dataframe's rows or columns such as summary statistics. In the next chapter we are going to take a look at advanced transformations."
      ],
      "metadata": {
        "id": "RcEXL7sKmO2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Advanced Transformations\n",
        "Advanced transformations are where PySpark really shines enabling us to execute very complex queries using simple syntax to extract valuable insights from our data. In this chapter we will see the power of methods such as `.groupBy()`, `.join()` especially in combination with more complex functions that are provided by the `functions` module. "
      ],
      "metadata": {
        "id": "xOWsFQy4cDM9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Why use Spark functions ?\n",
        "In general it is possible to use functions from other libraries such as `numpy` on Spark `DataFrame` objects, however this defeats the purpose of Spark which is its ability to optimize the performance of transformation pipelines due to lazy execution. \n",
        "\n",
        "This is why the `functions` exists which provides use with a copious amount of functions for all kinds of purposes.\n",
        "\n",
        "Suppose we want to take the mean petal length of the virginica species. We can reuse the DataFrame `df_virginica` that we created before.\n"
      ],
      "metadata": {
        "id": "Ef25TfNJkhfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import mean\n",
        "virginica_mean_petalLength = df_virginica.select(mean(\"petalLength\"))\n",
        "# Execute pipeline.\n",
        "virginica_mean_petalLength = virginica_mean_petalLength.collect()\n",
        "print(f\"Type of virginica_mean_petalLength: {type(virginica_mean_petalLength[0])}\\n\")\n",
        "print(f\"Mean petal length of virginica species: {virginica_mean_petalLength[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxUxGZYJbnFc",
        "outputId": "1ad3ff54-921e-4233-c4ce-6a495fc32126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of virginica_mean_petalLength: <class 'pyspark.sql.types.Row'>\n",
            "\n",
            "Mean petal length of virginica species: Row(avg(petalLength)=5.561224489795917)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ndAXs7SsrGEL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
