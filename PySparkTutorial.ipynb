{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PySpark Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMWTby2/2HWpsdkYAFhcsVE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fuenfgeld/2022TeamADataEngineeringBC/blob/PySpark/PySparkTutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0kMaqIMwKk-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1876839-67e2-4ed7-99b0-23de8a4f0730"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.1)\n",
            "Requirement already satisfied: py4j==0.10.9.3 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.3)\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -cq https://raw.githubusercontent.com/Fuenfgeld/2022TeamADataEngineeringBC/ca4b2ecc9e9ee242037d11c27edd4f4ad770e7ee/iris.json"
      ],
      "metadata": {
        "id": "AWFXHurvlAP7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -cq https://raw.githubusercontent.com/Fuenfgeld/2022TeamADataEngineeringBC/PySpark/iris2.json"
      ],
      "metadata": {
        "id": "UwhFqG5OpkrE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Loading Data\n",
        "\n",
        "Before we can analyze data we have to load it into our working environment. PySpark has a lot of functions that can deal with all kinds of formats from `.csv` to `.json`. The basic unit of data storage in PySpark is the so called `DataFrame` class."
      ],
      "metadata": {
        "id": "FjsR9GZemXN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "WQMsUJrIM3-I"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = spark.read.option(\"multiline\",True).json('iris.json')\n",
        "print(f\"Object Type: {type(df1)}\\n\")\n",
        "print(\"Column Info:\")\n",
        "df1.printSchema()\n",
        "print(\"Overview Dataframe:\")\n",
        "df1.show(10)"
      ],
      "metadata": {
        "id": "Kc5OYn6VNa9S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4e50504-acbe-4a32-b12b-5579afa031f1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object Type: <class 'pyspark.sql.dataframe.DataFrame'>\n",
            "\n",
            "Column Info:\n",
            "root\n",
            " |-- petalLength: double (nullable = true)\n",
            " |-- petalWidth: double (nullable = true)\n",
            " |-- sepalLength: double (nullable = true)\n",
            " |-- sepalWidth: double (nullable = true)\n",
            " |-- species: string (nullable = true)\n",
            "\n",
            "Overview Dataframe:\n",
            "+-----------+----------+-----------+----------+-------+\n",
            "|petalLength|petalWidth|sepalLength|sepalWidth|species|\n",
            "+-----------+----------+-----------+----------+-------+\n",
            "|        1.4|       0.2|        5.1|       3.5| setosa|\n",
            "|        1.4|       0.2|        4.9|       3.0| setosa|\n",
            "|        1.3|       0.2|        4.7|       3.2| setosa|\n",
            "|        1.5|       0.2|        4.6|       3.1| setosa|\n",
            "|        1.4|       0.2|        5.0|       3.6| setosa|\n",
            "|        1.7|       0.4|        5.4|       3.9| setosa|\n",
            "|        1.4|       0.3|        4.6|       3.4| setosa|\n",
            "|        1.5|       0.2|        5.0|       3.4| setosa|\n",
            "|        1.4|       0.2|        4.4|       2.9| setosa|\n",
            "|        1.5|       0.1|        4.9|       3.1| setosa|\n",
            "+-----------+----------+-----------+----------+-------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1.select(\"petalLength\").describe().show()"
      ],
      "metadata": {
        "id": "YjXEYeYSPu6J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "088fa863-1651-47bc-9dc3-2962dcb341fa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+\n",
            "|summary|       petalLength|\n",
            "+-------+------------------+\n",
            "|  count|               150|\n",
            "|   mean|3.7580000000000027|\n",
            "| stddev|1.7652982332594662|\n",
            "|    min|               1.0|\n",
            "|    max|               6.9|\n",
            "+-------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Basic transformations\n",
        "Some of the most basic functionalities of tables are that we can access specific chunks of the tables rows and columns as well as new rows and columns."
      ],
      "metadata": {
        "id": "92lmS82aSNDx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`DataFrame.collect()` collects the distributed data to the driver side as the local data in Python. Note that this can throw an out-of-memory error when the dataset is too large to fit in the driver side because it collects all the data from executors to the driver side."
      ],
      "metadata": {
        "id": "Q_npo5fxSRAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "local_df1 = df1.collect()\n",
        "local_df1[0]"
      ],
      "metadata": {
        "id": "wqjHubRZSVJg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19c15f80-0fab-4ac0-b618-8efe755d3517"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(petalLength=1.4, petalWidth=0.2, sepalLength=5.1, sepalWidth=3.5, species='setosa')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want to access specific columns we can use `.NameOfColumn`"
      ],
      "metadata": {
        "id": "Z0jSHnkiYZ6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "petalLength = df1.petalLength\n",
        "petalWidth = df1.petalWidth\n",
        "df1.select(petalLength, petalWidth).show(5)"
      ],
      "metadata": {
        "id": "PxgmXX92Yi-H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "783f522b-6dfe-4d17-fba9-589e9e34c149"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+\n",
            "|petalLength|petalWidth|\n",
            "+-----------+----------+\n",
            "|        1.4|       0.2|\n",
            "|        1.4|       0.2|\n",
            "|        1.3|       0.2|\n",
            "|        1.5|       0.2|\n",
            "|        1.4|       0.2|\n",
            "+-----------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we have similar datasets from multiple sources. Wouldn't it be practical to combine them into one table ? `pyspark` provides such a funcionality via the `.union()` method which is equivalent to `UNION ALL` in SQL."
      ],
      "metadata": {
        "id": "yOmxStmxdPYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = spark.read.json('iris2.json')\n",
        "df2.show()\n",
        "df1.union(df2)"
      ],
      "metadata": {
        "id": "Ei_V2RL5ZhSj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfd1a56f-46ff-4acf-b31d-23d5d15444e9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+-----------+----------+---------+\n",
            "|petalLength|petalWidth|sepalLength|sepalWidth|  species|\n",
            "+-----------+----------+-----------+----------+---------+\n",
            "|        5.1|       1.8|        5.9|       3.0|virginica|\n",
            "+-----------+----------+-----------+----------+---------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[petalLength: double, petalWidth: double, sepalLength: double, sepalWidth: double, species: string]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In case we want to add columns we can do so via the `.withColumn()` method. Note that we have to specify the name of the column which is in this case `petalSum`. Usually the new column is a function of one or more of the old columns. "
      ],
      "metadata": {
        "id": "heeiHT59jaXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_extraCol = df1.withColumn('newColumn', df1.petalWidth + df1.petalLength)\n",
        "df_extraCol.show(5)"
      ],
      "metadata": {
        "id": "7XRQiJVSij7m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9419a3c1-14b2-4bb7-d388-18ff3d6e79b6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+-----------+----------+-------+------------------+\n",
            "|petalLength|petalWidth|sepalLength|sepalWidth|species|         newColumn|\n",
            "+-----------+----------+-----------+----------+-------+------------------+\n",
            "|        1.4|       0.2|        5.1|       3.5| setosa|1.5999999999999999|\n",
            "|        1.4|       0.2|        4.9|       3.0| setosa|1.5999999999999999|\n",
            "|        1.3|       0.2|        4.7|       3.2| setosa|               1.5|\n",
            "|        1.5|       0.2|        4.6|       3.1| setosa|               1.7|\n",
            "|        1.4|       0.2|        5.0|       3.6| setosa|1.5999999999999999|\n",
            "+-----------+----------+-----------+----------+-------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The name `'newColumn'` isn't really informative. It's therefore hard for the user to deduce that is it the sum of `'petalWidth'` and `'petalLength'`. So why not rename it to something more indicative ? We can do this via the `withColumnRenamed` method."
      ],
      "metadata": {
        "id": "Pf1MnUAcI7M5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_extraCol = df_extraCol.withColumnRenamed('newColumn','petalSum')\n",
        "df_extraCol.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHeB_op4JtMF",
        "outputId": "2d57770f-2b05-4953-84b2-2873bddc6beb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+-----------+----------+-------+------------------+\n",
            "|petalLength|petalWidth|sepalLength|sepalWidth|species|          petalSum|\n",
            "+-----------+----------+-----------+----------+-------+------------------+\n",
            "|        1.4|       0.2|        5.1|       3.5| setosa|1.5999999999999999|\n",
            "|        1.4|       0.2|        4.9|       3.0| setosa|1.5999999999999999|\n",
            "|        1.3|       0.2|        4.7|       3.2| setosa|               1.5|\n",
            "|        1.5|       0.2|        4.6|       3.1| setosa|               1.7|\n",
            "|        1.4|       0.2|        5.0|       3.6| setosa|1.5999999999999999|\n",
            "+-----------+----------+-----------+----------+-------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to get rid of our new column `.drop()` can be used. In contrast to `.select()`, this method removes the specified column completely instead of returning it as slice ot the table.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NKioCZzjlL-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df_extraCol.drop(df_extraCol.petalSum)\n",
        "df1.show(5)"
      ],
      "metadata": {
        "id": "Xp8ScsA-ixVF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63ab893d-32d2-4ab9-de03-6fc7fb6aa772"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+-----------+----------+-------+\n",
            "|petalLength|petalWidth|sepalLength|sepalWidth|species|\n",
            "+-----------+----------+-----------+----------+-------+\n",
            "|        1.4|       0.2|        5.1|       3.5| setosa|\n",
            "|        1.4|       0.2|        4.9|       3.0| setosa|\n",
            "|        1.3|       0.2|        4.7|       3.2| setosa|\n",
            "|        1.5|       0.2|        4.6|       3.1| setosa|\n",
            "|        1.4|       0.2|        5.0|       3.6| setosa|\n",
            "+-----------+----------+-----------+----------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes rows also contain entries that make dealing with our data more difficult or lower its quality. Two examples come to mind: Duplicate entries could bias introduce bias into our data which negatively impacts the performance of a lot of machine learning algorithms.\n",
        "\n",
        "The second example would be null entries which might render some rows useless due to the fact that most algorithms generally can't handle such entries. Luckily PySpark provides us with two methods `.dropna()` and `.dropDuplicates()` to get rid of such problematic rows.\n",
        "\n"
      ],
      "metadata": {
        "id": "o_9RPA4ZL7W9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df1.dropna()"
      ],
      "metadata": {
        "id": "22OiPKXUTVVd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df1.dropDuplicates() "
      ],
      "metadata": {
        "id": "lNGMK9rwR4rY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You learned how to perform some basic transformations of the table, but what if you want to look up values not based on indices but rather on criteria such as a certain column's entry being bigger than some threshold? In the next chapter we are going to take a look at how to select rows via user given conditions."
      ],
      "metadata": {
        "id": "RcEXL7sKmO2k"
      }
    }
  ]
}