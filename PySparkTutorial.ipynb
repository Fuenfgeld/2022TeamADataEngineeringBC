{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PySparkTutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "FjsR9GZemXN9",
        "FSLy8EIT9Qs1",
        "ClcImNjp1pRm",
        "-23HYBWb277h",
        "AvnZOVxV7K9t",
        "NKioCZzjlL-L",
        "o_9RPA4ZL7W9",
        "RtB5fpk0flQQ",
        "Ef25TfNJkhfs"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fuenfgeld/2022TeamADataEngineeringBC/blob/save-to-database/PySparkTutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0kMaqIMwKk-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a778552b-749b-4d79-a280-3bcf1b6481b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: py4j==0.10.9.3 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -cq https://raw.githubusercontent.com/Fuenfgeld/2022TeamADataEngineeringBC/ca4b2ecc9e9ee242037d11c27edd4f4ad770e7ee/iris.json"
      ],
      "metadata": {
        "id": "AWFXHurvlAP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -cq https://raw.githubusercontent.com/Fuenfgeld/2022TeamADataEngineeringBC/PySpark/iris2.json"
      ],
      "metadata": {
        "id": "UwhFqG5OpkrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Loading Data\n",
        "\n",
        "Before we can analyze data we have to load it into our working environment. PySpark has a lot of functions that can deal with all kinds of formats from `.csv` to `.json`. The basic unit of data storage in PySpark is the so called `DataFrame` class."
      ],
      "metadata": {
        "id": "FjsR9GZemXN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "WQMsUJrIM3-I"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = spark.read.option(\"multiline\",True).json('iris.json')\n",
        "print(f\"Object Type: {type(df1)}\\n\")\n",
        "print(\"Column Info:\")\n",
        "df1.printSchema()\n",
        "print(\"Summary Statistics of columns:\")\n",
        "df1.describe().show()\n",
        "print(\"Overview Dataframe:\")\n",
        "df1.show(10)"
      ],
      "metadata": {
        "id": "Kc5OYn6VNa9S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44476800-9101-45d7-e836-ad72833535bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object Type: <class 'pyspark.sql.dataframe.DataFrame'>\n",
            "\n",
            "Column Info:\n",
            "root\n",
            " |-- petalLength: double (nullable = true)\n",
            " |-- petalWidth: double (nullable = true)\n",
            " |-- sepalLength: double (nullable = true)\n",
            " |-- sepalWidth: double (nullable = true)\n",
            " |-- species: string (nullable = true)\n",
            "\n",
            "Summary Statistics of columns:\n",
            "+-------+------------------+------------------+------------------+-------------------+---------+\n",
            "|summary|       petalLength|        petalWidth|       sepalLength|         sepalWidth|  species|\n",
            "+-------+------------------+------------------+------------------+-------------------+---------+\n",
            "|  count|               150|               150|               150|                150|      150|\n",
            "|   mean|3.7580000000000027| 1.199333333333334| 5.843333333333335|  3.057333333333334|     null|\n",
            "| stddev|1.7652982332594662|0.7622376689603467|0.8280661279778637|0.43586628493669793|     null|\n",
            "|    min|               1.0|               0.1|               4.3|                2.0|   setosa|\n",
            "|    max|               6.9|               2.5|               7.9|                4.4|virginica|\n",
            "+-------+------------------+------------------+------------------+-------------------+---------+\n",
            "\n",
            "Overview Dataframe:\n",
            "+-----------+----------+-----------+----------+-------+\n",
            "|petalLength|petalWidth|sepalLength|sepalWidth|species|\n",
            "+-----------+----------+-----------+----------+-------+\n",
            "|        1.4|       0.2|        5.1|       3.5| setosa|\n",
            "|        1.4|       0.2|        4.9|       3.0| setosa|\n",
            "|        1.3|       0.2|        4.7|       3.2| setosa|\n",
            "|        1.5|       0.2|        4.6|       3.1| setosa|\n",
            "|        1.4|       0.2|        5.0|       3.6| setosa|\n",
            "|        1.7|       0.4|        5.4|       3.9| setosa|\n",
            "|        1.4|       0.3|        4.6|       3.4| setosa|\n",
            "|        1.5|       0.2|        5.0|       3.4| setosa|\n",
            "|        1.4|       0.2|        4.4|       2.9| setosa|\n",
            "|        1.5|       0.1|        4.9|       3.1| setosa|\n",
            "+-----------+----------+-----------+----------+-------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Basic transformations\n",
        "Some of the most basic functionalities of tables are that we can access specific chunks of the table's rows and columns as well as create new rows and columns."
      ],
      "metadata": {
        "id": "92lmS82aSNDx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Accessing Rows\n",
        "\n",
        "Since Spark was concieved to work with distributed data there is no simple way to access rows at will.\n",
        "\n",
        "If you want to do so anyways you have the possibility to pull the data onto your local node.\n",
        "\n",
        "`DataFrame.collect()` collects the distributed data to the driver side as local data in Python. Note that this can throw an out-of-memory error when the dataset is too large to fit in the driver side because it collects all the data from executors to the driver side."
      ],
      "metadata": {
        "id": "FSLy8EIT9Qs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns list of Row objects\n",
        "local_df1 = df1.collect()\n",
        "print(f\"Type of entries: {type(local_df1[0])}\\n\")\n",
        "print(f\"Entries: {local_df1[:5]}\")"
      ],
      "metadata": {
        "id": "wqjHubRZSVJg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6b68cad-9e6f-4d4b-cdac-12e6445c3583"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of entries: <class 'pyspark.sql.types.Row'>\n",
            "\n",
            "Entries: [Row(petalLength=1.4, petalWidth=0.2, sepalLength=5.1, sepalWidth=3.5, species='setosa'), Row(petalLength=1.4, petalWidth=0.2, sepalLength=4.9, sepalWidth=3.0, species='setosa'), Row(petalLength=1.3, petalWidth=0.2, sepalLength=4.7, sepalWidth=3.2, species='setosa'), Row(petalLength=1.5, petalWidth=0.2, sepalLength=4.6, sepalWidth=3.1, species='setosa'), Row(petalLength=1.4, petalWidth=0.2, sepalLength=5.0, sepalWidth=3.6, species='setosa')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Accessing Columns\n",
        "\n",
        "Accessing columns doesn't come with the difficulties associated with handling rows. If we want to get specific columns we can simply do so through the `.select()` method. "
      ],
      "metadata": {
        "id": "ClcImNjp1pRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.select(\"petalLength\").show(5)"
      ],
      "metadata": {
        "id": "YjXEYeYSPu6J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84f18cec-75dc-469d-8dd4-e755124be62c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|petalLength|\n",
            "+-----------+\n",
            "|        1.4|\n",
            "|        1.4|\n",
            "|        1.3|\n",
            "|        1.5|\n",
            "|        1.4|\n",
            "+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is also possible to choose mutiple columns. Notice that we can adress our columns with `DataFrame.NameOfColumn` instead of `\"NameOfColumn\"`."
      ],
      "metadata": {
        "id": "Z0jSHnkiYZ6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "petalLength = df1.petalLength\n",
        "petalWidth = df1.petalWidth\n",
        "df1.select(petalLength, petalWidth).show(5)"
      ],
      "metadata": {
        "id": "PxgmXX92Yi-H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f99c9043-775a-46a8-e60a-0cf230c5b0d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+\n",
            "|petalLength|petalWidth|\n",
            "+-----------+----------+\n",
            "|        1.4|       0.2|\n",
            "|        1.4|       0.2|\n",
            "|        1.3|       0.2|\n",
            "|        1.5|       0.2|\n",
            "|        1.4|       0.2|\n",
            "+-----------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. Concatenating DataFrames"
      ],
      "metadata": {
        "id": "-23HYBWb277h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we have a dataset that is split into multiple DataFrames. Wouldn't it be practical to combine them into one table ? `pyspark` provides such a funcionality via the `.union()` method."
      ],
      "metadata": {
        "id": "yOmxStmxdPYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = spark.read.json('iris2.json')\n",
        "df2.show()\n",
        "df1.union(df2)"
      ],
      "metadata": {
        "id": "Ei_V2RL5ZhSj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b162e299-c5c5-493c-9c8b-6b71a90bca09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+-----------+----------+---------+\n",
            "|petalLength|petalWidth|sepalLength|sepalWidth|  species|\n",
            "+-----------+----------+-----------+----------+---------+\n",
            "|        5.1|       1.8|        5.9|       3.0|virginica|\n",
            "+-----------+----------+-----------+----------+---------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[petalLength: double, petalWidth: double, sepalLength: double, sepalWidth: double, species: string]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Adding Columns\n",
        "\n",
        "In case we want to add columns we can do so via the `.withColumn()` method. Note that we have to specify the name of the column which is in this case `petalSum`. Usually the new column is a function of one or more of the old columns. "
      ],
      "metadata": {
        "id": "AvnZOVxV7K9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_extraCol = df1.withColumn('newColumn', df1.petalWidth + df1.petalLength)\n",
        "df_extraCol.show(5)"
      ],
      "metadata": {
        "id": "7XRQiJVSij7m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ec38faa-c950-46fe-f594-666720f5f79f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+-----------+----------+-------+------------------+\n",
            "|petalLength|petalWidth|sepalLength|sepalWidth|species|         newColumn|\n",
            "+-----------+----------+-----------+----------+-------+------------------+\n",
            "|        1.4|       0.2|        5.1|       3.5| setosa|1.5999999999999999|\n",
            "|        1.4|       0.2|        4.9|       3.0| setosa|1.5999999999999999|\n",
            "|        1.3|       0.2|        4.7|       3.2| setosa|               1.5|\n",
            "|        1.5|       0.2|        4.6|       3.1| setosa|               1.7|\n",
            "|        1.4|       0.2|        5.0|       3.6| setosa|1.5999999999999999|\n",
            "+-----------+----------+-----------+----------+-------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The name `'newColumn'` isn't really informative. It's therefore hard for the user to deduce that is it the sum of `'petalWidth'` and `'petalLength'`. So why not rename it to something more indicative ? We can do this via the `.withColumnRenamed()` method."
      ],
      "metadata": {
        "id": "Pf1MnUAcI7M5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_extraCol = df_extraCol.withColumnRenamed('newColumn','petalSum')\n",
        "df_extraCol.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHeB_op4JtMF",
        "outputId": "4c20d380-39b1-429d-a029-35befff12045"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+-----------+----------+-------+------------------+\n",
            "|petalLength|petalWidth|sepalLength|sepalWidth|species|          petalSum|\n",
            "+-----------+----------+-----------+----------+-------+------------------+\n",
            "|        1.4|       0.2|        5.1|       3.5| setosa|1.5999999999999999|\n",
            "|        1.4|       0.2|        4.9|       3.0| setosa|1.5999999999999999|\n",
            "|        1.3|       0.2|        4.7|       3.2| setosa|               1.5|\n",
            "|        1.5|       0.2|        4.6|       3.1| setosa|               1.7|\n",
            "|        1.4|       0.2|        5.0|       3.6| setosa|1.5999999999999999|\n",
            "+-----------+----------+-----------+----------+-------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5. Removing Columns\n",
        "\n",
        "In order to get rid of our new column `.drop()` can be used. In contrast to `.select()`, this method removes the specified column completely instead of returning it as slice ot the table.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NKioCZzjlL-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df_extraCol.drop(df_extraCol.petalSum)\n",
        "df1.show(5)"
      ],
      "metadata": {
        "id": "Xp8ScsA-ixVF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54344dc7-9a6a-410f-f9c5-bfbb8fd1da5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+-----------+----------+-------+\n",
            "|petalLength|petalWidth|sepalLength|sepalWidth|species|\n",
            "+-----------+----------+-----------+----------+-------+\n",
            "|        1.4|       0.2|        5.1|       3.5| setosa|\n",
            "|        1.4|       0.2|        4.9|       3.0| setosa|\n",
            "|        1.3|       0.2|        4.7|       3.2| setosa|\n",
            "|        1.5|       0.2|        4.6|       3.1| setosa|\n",
            "|        1.4|       0.2|        5.0|       3.6| setosa|\n",
            "+-----------+----------+-----------+----------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6. Basic Data Cleaning\n",
        "\n",
        "Just as in the hospital, hygiene is of great importance to working with data, sometimes rows contain entries that make dealing with our data more difficult or lower its quality (information pollution). Two examples come to mind: Duplicate entries could bias introduce into our data which negatively impacts the performance of a lot of machine learning algorithms.\n",
        "\n",
        "The second example would be null entries which might render some rows useless due to the fact that most algorithms generally can't handle such entries. Luckily PySpark provides us with two methods `.dropna()` and `.dropDuplicates()` to get rid of such problematic rows.\n",
        "\n"
      ],
      "metadata": {
        "id": "o_9RPA4ZL7W9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df1.dropna()"
      ],
      "metadata": {
        "id": "22OiPKXUTVVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df1.dropDuplicates() "
      ],
      "metadata": {
        "id": "lNGMK9rwR4rY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although our dataframe is now free of unwanted entries we might still want to put further restrictions on the data we want to keep. "
      ],
      "metadata": {
        "id": "Em-5JUVf82Kg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.7. Conditional Selection of Rows.\n",
        "\n",
        "In 2.1. we explained that directly accessing rows of a DataFrame comes with some caveats, it is however possible to indirectly access rows without pulling all the data onto your local node. This is done via conditional selection where we select rows based on user given conditions via the `.filter()` method. This means however that we don't know which rows we will obtain in the end, hence why we speak of indirect access.\n",
        "\n",
        "Let's say we want to get only the flowers of type `\"virginica\"` we then have to write the following:"
      ],
      "metadata": {
        "id": "RtB5fpk0flQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_virginica = df1.filter(df1.species == \"virginica\")\n",
        "df_virginica.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XN6A0o-ziDhm",
        "outputId": "cea133a3-525a-4be7-fb4b-b09ac8a387e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+-----------+----------+---------+\n",
            "|petalLength|petalWidth|sepalLength|sepalWidth|  species|\n",
            "+-----------+----------+-----------+----------+---------+\n",
            "|        6.0|       1.8|        7.2|       3.2|virginica|\n",
            "|        5.6|       2.1|        6.4|       2.8|virginica|\n",
            "|        5.1|       2.3|        6.9|       3.1|virginica|\n",
            "|        6.1|       2.5|        7.2|       3.6|virginica|\n",
            "|        5.7|       2.3|        6.9|       3.2|virginica|\n",
            "+-----------+----------+-----------+----------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.8 Alter data based using Lambda\n",
        "\n",
        "Using the `map` function columns and the full structure can be altered using Lambdas."
      ],
      "metadata": {
        "id": "-KQh0yhFdgTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import types, functions\n",
        "\n",
        "data = [\n",
        "        ('Max', 'Mustermann', 'm', '10', '1954', '2020'),\n",
        "        ('Erika', 'Mustermann', 'w', '12', '1994', None)\n",
        "        ]\n",
        "schema = ['firstname', 'lastname', 'gender', 'salary', 'birthyear', 'deathyear']\n",
        "\n",
        "frame = spark.createDataFrame(data = data, schema = schema)\n",
        "frame.show()\n",
        "\n",
        "# To cast or alter a column, just override it\n",
        "parsed = frame.withColumn('salary', functions.col('salary').cast(types.IntegerType()))\n",
        "\n",
        "# Single column transformations\n",
        "doubled = parsed.withColumn('salary', functions.col('salary') * 2)\n",
        "\n",
        "# Conditional replacements are possible using functions\n",
        "replaceNullValue = doubled.withColumn(\n",
        "    'deathyear', \n",
        "    functions.when(functions.col('deathyear').isNull(), '2022')\n",
        "    .otherwise(functions.col('deathyear'))\n",
        ")\n",
        "\n",
        "# Spark has its own mapping language which can be used in withColumn\n",
        "withAge = replaceNullValue.withColumn('age', functions.col('deathyear') - functions.col('birthyear'))\n",
        "\n",
        "# To replace a value with a value in a dictionary, you replace the value on the whole\n",
        "# dataset and restrict the changes to the columns in which it should be replaced\n",
        "genders = { 'm': 'male', 'w': 'female' }\n",
        "withGender = withAge.replace(genders, subset='gender')\n",
        "withGender.show()\n",
        "\n",
        "# Lambdas can also be used. They are slower but more powerful and can alter the schema\n",
        "converted = (withGender.rdd\n",
        "  .map(lambda row: (row[0] + ' ' + row[1], row[2], row[3], row[6]))\n",
        "  .toDF(['name', 'gender', 'salary', 'age'])\n",
        ")\n",
        "converted.show()\n",
        "\n",
        "# The transformations can also be written functionally\n",
        "functional = (spark.createDataFrame(data = data, schema = schema)\n",
        "    .withColumn('salary', functions.col('salary').cast(types.IntegerType()))\n",
        "    .withColumn('salary', functions.col('salary') * 2)\n",
        "    .withColumn(\n",
        "        'deathyear', \n",
        "        functions.when(functions.col('deathyear').isNull(), '2022')\n",
        "        .otherwise(functions.col('deathyear'))\n",
        "    )\n",
        "    .withColumn('age', functions.col('deathyear') - functions.col('birthyear'))\n",
        "    .replace(genders, subset='gender')\n",
        "    .rdd\n",
        "    .map(lambda row: (row[0] + ' ' + row[1], row[2], row[3], row[6]))\n",
        "    .toDF(['name', 'gender', 'salary', 'age'])\n",
        ")\n",
        "functional.show()"
      ],
      "metadata": {
        "id": "y5tbJpJ2d1na",
        "outputId": "4dc9ef68-662c-4c5e-a3d2-5e5d84d9a3e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+------+------+---------+---------+\n",
            "|firstname|  lastname|gender|salary|birthyear|deathyear|\n",
            "+---------+----------+------+------+---------+---------+\n",
            "|      Max|Mustermann|     m|    10|     1954|     2020|\n",
            "|    Erika|Mustermann|     w|    12|     1994|     null|\n",
            "+---------+----------+------+------+---------+---------+\n",
            "\n",
            "+---------+----------+------+------+---------+---------+----+\n",
            "|firstname|  lastname|gender|salary|birthyear|deathyear| age|\n",
            "+---------+----------+------+------+---------+---------+----+\n",
            "|      Max|Mustermann|  male|    20|     1954|     2020|66.0|\n",
            "|    Erika|Mustermann|female|    24|     1994|     2022|28.0|\n",
            "+---------+----------+------+------+---------+---------+----+\n",
            "\n",
            "+----------------+------+------+----+\n",
            "|            name|gender|salary| age|\n",
            "+----------------+------+------+----+\n",
            "|  Max Mustermann|  male|    20|66.0|\n",
            "|Erika Mustermann|female|    24|28.0|\n",
            "+----------------+------+------+----+\n",
            "\n",
            "+----------------+------+------+----+\n",
            "|            name|gender|salary| age|\n",
            "+----------------+------+------+----+\n",
            "|  Max Mustermann|  male|    20|66.0|\n",
            "|Erika Mustermann|female|    24|28.0|\n",
            "+----------------+------+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Join data based on key"
      ],
      "metadata": {
        "id": "5coT1v-8vFNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "people = spark.createDataFrame(data = [( 'Max', 1 ), ( 'Erika', 0 )],\n",
        "                               schema = ['name', 'cityId'])\n",
        "people.show()\n",
        "cities = spark.createDataFrame(data=[(0, 'Mannheim'), (1, 'Frankfurt')],\n",
        "                               schema=['cityId', 'city'])\n",
        "cities.show()\n",
        "\n",
        "combined = people.join(cities, ['cityId'], \"inner\").drop('cityId')\n",
        "combined.show()"
      ],
      "metadata": {
        "id": "FqxzQ9EmvEoX",
        "outputId": "1f8b7631-e460-4b1a-a6ff-7a2a249b608b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+\n",
            "| name|cityId|\n",
            "+-----+------+\n",
            "|  Max|     1|\n",
            "|Erika|     0|\n",
            "+-----+------+\n",
            "\n",
            "+------+---------+\n",
            "|cityId|     city|\n",
            "+------+---------+\n",
            "|     0| Mannheim|\n",
            "|     1|Frankfurt|\n",
            "+------+---------+\n",
            "\n",
            "+-----+---------+\n",
            "| name|     city|\n",
            "+-----+---------+\n",
            "|Erika| Mannheim|\n",
            "|  Max|Frankfurt|\n",
            "+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.8. Conclusion\n",
        "\n",
        "You learned how to perform some basic transformations of the table, but maybe you also want to apply more complex functions to the dataframe's rows or columns such as summary statistics. In the next chapter we are going to take a look at advanced transformations."
      ],
      "metadata": {
        "id": "RcEXL7sKmO2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Advanced Transformations\n",
        "Advanced transformations are where PySpark really shines enabling us to execute very complex queries using simple syntax to extract valuable insights from our data. In this chapter we will see the power of methods such as `.groupBy()`, `.join()` especially in combination with more complex functions that are provided by the `functions` module. "
      ],
      "metadata": {
        "id": "xOWsFQy4cDM9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Why use Spark functions ?\n",
        "In general it is possible to use functions from other libraries such as `numpy` on Spark `DataFrame` objects, however this defeats the purpose of Spark which is its ability to optimize the performance of transformation pipelines due to lazy execution. \n",
        "\n",
        "This is why the `functions` exists which provides use with a copious amount of functions for all kinds of purposes.\n",
        "\n",
        "Suppose we want to take the mean petal length of the virginica species. We can reuse the DataFrame `df_virginica` that we created before.\n"
      ],
      "metadata": {
        "id": "Ef25TfNJkhfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import mean\n",
        "virginica_mean_petalLength = df_virginica.select(mean(\"petalLength\"))\n",
        "# Execute pipeline.\n",
        "virginica_mean_petalLength = virginica_mean_petalLength.collect()\n",
        "print(f\"Type of virginica_mean_petalLength: {type(virginica_mean_petalLength[0])}\\n\")\n",
        "print(f\"Mean petal length of virginica species: {virginica_mean_petalLength[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxUxGZYJbnFc",
        "outputId": "1ad3ff54-921e-4233-c4ce-6a495fc32126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of virginica_mean_petalLength: <class 'pyspark.sql.types.Row'>\n",
            "\n",
            "Mean petal length of virginica species: Row(avg(petalLength)=5.561224489795917)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 Save results to database\n",
        "\n",
        "The easiest way to save the data is to convert the dataframe to a pandas dataframe and have pandas generate all SQL statements by itself."
      ],
      "metadata": {
        "id": "f19m3l4TyN5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "connection = sqlite3.connect('my-database.sqlite')\n",
        "\n",
        "# Please note that tuples with a single value always end with a comma.\n",
        "# e.g. ('Max', ). If that comma does not exist, the value will be seen as a string\n",
        "# instead of a tuple.\n",
        "frame = spark.createDataFrame(data=[('Max', ), ('Erika', )], schema=['name'])\n",
        "frame.toPandas().to_sql('table_name', connection, if_exists='replace', index=True)\n",
        "\n",
        "print(connection.execute('SELECT * FROM table_name;').fetchall())\n",
        "\n",
        "connection.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVjVbGdTzdUU",
        "outputId": "9ab0c523-c1c6-4881-c0c9-67b784238bf0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 'Max'), (1, 'Erika')]\n"
          ]
        }
      ]
    }
  ]
}