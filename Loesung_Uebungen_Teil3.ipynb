{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fuenfgeld/2022TeamADataEngineeringBC/blob/main/Loesung_Uebungen_Teil3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zopt-B7jUV16"
      },
      "source": [
        "# Vorbereitung einlesen der Daten \n",
        "\n",
        "In diesem Abschnitt müssen Sie nichts machen. Dieser dient nur für das Einlesen und die Bereitstellung der Daten für die folgenden Übungen. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GfC-OQBE_bh",
        "outputId": "4036f817-93cf-48c3-d17d-8b95b5fcda73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: pysqlite3 in /usr/local/lib/python3.7/dist-packages (0.4.7)\n",
            "Requirement already satisfied: py4j==0.10.9.3 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "# Bibliotheken einmalig installieren\n",
        "!pip install pyspark pandas pysqlite3 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-23uk-88H6R",
        "outputId": "29821cc0-c3da-4fc1-8458-0c4017039224"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-05-12 11:10:53--  https://github.com/Fuenfgeld/2022TeamADataEngineeringBC/blob/36-pr%C3%A4sentation-fortgeschrittene-transaktionen/Pr%C3%A4sentationen/03-Fortgeschrittene%20Transformationen/create_data_advacend.py\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘create_data_advacend.py’\n",
            "\n",
            "\rcreate_data_advacen     [<=>                 ]       0  --.-KB/s               \rcreate_data_advacen     [ <=>                ] 296.55K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2022-05-12 11:10:53 (5.21 MB/s) - ‘create_data_advacend.py’ saved [303669]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Angabe der Daten für den Pfad\n",
        "!wget -O create_data_advacend.py https://raw.githubusercontent.com/Fuenfgeld/2022TeamADataEngineeringBC/36-pr%C3%A4sentation-fortgeschrittene-transaktionen/Pr%C3%A4sentationen/03-Fortgeschrittene%20Transformationen/create_data_advacend.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wrNz_wMFTYI",
        "outputId": "92ce41d4-2f4b-4d9c-df6c-c502616e6b4c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Import der nötigen Packete\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import pyspark.sql.functions as func\n",
        "import os\n",
        "\n",
        "os.system(\"python3 create_data_advacend.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcR_IIdyVAwo"
      },
      "outputs": [],
      "source": [
        "## (Py)Spark starten\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "ZS1ALDxs8KlI",
        "outputId": "f9ede37c-0cd5-4598-8c93-fa5e2210fdc1"
      },
      "outputs": [
        {
          "ename": "AnalysisException",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-8c59a79fe7f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Datei Corn einlesen & in Spark-Dataframe schreiben\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_corn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multiline\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Corn.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, allowNonNumericNumbers, modifiedBefore, modifiedAfter)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/content/Corn.json"
          ]
        }
      ],
      "source": [
        "# Datei Corn einlesen & in Spark-Dataframe schreiben\n",
        "df_corn = spark.read.option(\"multiline\",True).json('Corn.json').show()\n",
        "df_corn.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "YP7ra-l3WSKr",
        "outputId": "4d2c48fa-1631-4c21-c850-33070e17ffd9"
      },
      "outputs": [
        {
          "ename": "OperationalError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-840b899bbd48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Tabelle Fields ausgeben\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcursor_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SELECT * FROM fields'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOperationalError\u001b[0m: no such table: fields"
          ]
        }
      ],
      "source": [
        "## Datenbankdaten in Dataframe schreiben\n",
        "connection_obj = sqlite3.connect('Diddly_Squat_Farm.db')\n",
        "cursor_obj = connection_obj.cursor()\n",
        "\n",
        "# Tabelle Fields ausgeben\n",
        "for row in cursor_obj.execute('SELECT * FROM fields'):\n",
        "        print(row)\n",
        "\n",
        "# Tabelle Fields in Dataframe df_fields schreiben\n",
        "df_fields = pd.read_sql_query(\"SELECT * FROM fields\", connection_obj)\n",
        "print('\\n\\nDies ist das erzeugte Dataframe:\\n\\n', df_fields)\n",
        "\n",
        "connection_obj.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkH8SqGOWVQe"
      },
      "outputs": [],
      "source": [
        "#DataFrame für Datenbank erzeugen\n",
        "df_fields= spark.createDataFrame(df_fields)\n",
        "df_fields.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQTRNXS1Vd1l"
      },
      "source": [
        "Nachdem nun alle Daten geladen wurden können Sie anschließend mit den Übungen beginnen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPlrbGG7CmG1"
      },
      "source": [
        "#Übungen für Fortgeschrittene \n",
        "\n",
        "In dieser Übung müssen Sie Jeremy tatkräftig unter die Arme greifen. \n",
        "Nachdem wir bereits Ihm bereits bei dem Anbau des Obst und Gemüse geholen haben, müssen Sie nun für in die Daten für das Getreide auswerten.\n",
        "\n",
        "Hierfür müssen Sie folgend vorgehen:\n",
        "\n",
        "1.   Helfen Sie Jeremy dabei sich einen Überblick über die Daten zu verschaffen in dem Sie JOIN verwerden.\n",
        "2.   Berechnen Sie für die jeweilge Getreideart den Wasserverbrauch, Ertrag pro Fläche sowie die Einnahmen.\n",
        "\n",
        "\n",
        "\n",
        "__________________________________________________________________________\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOP_N92hHCDy"
      },
      "source": [
        "Um Jeremy helfen zu können verschaffen Sie sich zunächst einen Überblick über die Daten."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "GawHQ1z_G9I6",
        "outputId": "fed17103-0521-47f5-dfd9-7e8b9162fe6e"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-5b495bee736b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_fields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_corn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_fields' is not defined"
          ]
        }
      ],
      "source": [
        "df_fields.show(10)\n",
        "df_corn.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uosHkloqBu8"
      },
      "source": [
        "Verknüpfen Sie anschließend die zwei Tabellen miteinander. Wählen Sie hierfür die geeignete Join-Bedingung. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_7gmwzzpWE_"
      },
      "outputs": [],
      "source": [
        "gesamt_join = df_corn.join(df_fields, df_corn.field ==  df_fields.field_id,\"inner\")\n",
        "gesamt_join.show(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae5T4zd7pk5F"
      },
      "source": [
        "Um die Wirtschaftlichkeit unseres Unternehmen zu prüfen, möchte Jeremy sich den gesamten Ertrag pro Fläch für die jeweiligen Getreidesorte wissen. Helfen Sie Jeremy dabei in dem Sie:\n",
        "\n",
        "+  Die Spalten des Ertrag pro m² mit der Fläche in m² multiplizieren um den gesamten Ertrag pro Fläche zu erhalten \n",
        "+ Erzeugen Sie eine neue Spalte im DataFrame für das Ergebnis\n",
        "+ Tipp: hierfür können Sie die Funcktion col der Sparkfunctions verwenden\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAnuWwdwpizi"
      },
      "outputs": [],
      "source": [
        "gesamt_join = gesamt_join.withColumn(\"yield_per_area\", func.col(\"yield_per_sqm\") * func.col(\"area_in_sqm\"))\n",
        "gesamt_join.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7AzyzkdwC5m"
      },
      "source": [
        "Nutzen Sie **groupby()** Bedingung sowie die Sparkfuntions **sum** um Jeremy eine Übersicht der Einnahmen, Wasserverbrauchs und dem Ertrag pro Fläche des jeweiligen Getreide zu verschaffen.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmz5ckT0vzrL"
      },
      "outputs": [],
      "source": [
        "gesamt_ertrag =  gesamt_join.groupBy(\"crop\").agg(func.sum(\"revenue\"), func.sum(\"yield_per_area\"), func.sum(\"water_consumption\"))\n",
        "gesamt_ertrag = gesamt_ertrag.select(func.col(\"crop\").alias(\"Crop\"),func.col(\"sum(revenue)\").alias(\"revenue\") , func.col(\"sum(water_consumption)\").alias(\"water_consumption\"), func.col(\"sum(yield_per_area)\").alias(\"yield_per_area\"))\n",
        "gesamt_ertrag.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFj7a_pnv1oO"
      },
      "source": [
        "Nachdem wir den Ertrag für die einzelnen Getreidesorten kennen, müssen wir Jeremy bei der Ermittelung des Gewinn je Lebensmittel unter die Arme greifen.\n",
        "\n",
        "+ Hierfür benötigen wir die zuvor ermittelten Liter je Sorte welche wir mit dem Wert von 0.2 Cent verrechnen. \n",
        " \n",
        "+ Anschließend müssen noch die Erwerbskosten für die  Pflanzensamen miteinbeziehen um unsere Produktionskosten zu erhalten.\n",
        "  >Kosten für Samen :\n",
        "  + Mais = 5 Cent\n",
        "  + Hafer = 10 Cent\n",
        "  + Roggen = 20 Cent \n",
        "\n",
        "\n",
        "+ Geben Sie das Ergebnis absteigend an. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjP287nU1D9j"
      },
      "source": [
        "Die folgende Codezeile können Sie einfach ausführen um mit den Übungen fortzufahren."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4RDD2cjohoE"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.window import Window as W\n",
        "rating = [5,10,20]\n",
        "seed_cost = spark.createDataFrame([(l,) for l in rating], ['cost_of_seed'])\n",
        "gesamt_ertrag = gesamt_ertrag.withColumn(\"idx\", func.monotonically_increasing_id())\n",
        "seed_cost = seed_cost.withColumn(\"idx\", func.monotonically_increasing_id())\n",
        "gesamt_ertrag.show()\n",
        "\n",
        "windowSpec = W.orderBy(\"idx\")\n",
        "gesamt_ertrag = gesamt_ertrag.withColumn(\"idx\", func.row_number().over(windowSpec))\n",
        "seed_cost = seed_cost.withColumn(\"idx\", func.row_number().over(windowSpec))\n",
        "seed_cost.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbuDGbNb7dMz"
      },
      "source": [
        "Nun sind Sie wieder gefragt. Joinen Sie die beiden Tabellen um mit der Ermittlung der Produktionskosten weiterzufahren."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7w-Vxf977Rtv"
      },
      "outputs": [],
      "source": [
        "gesamt_ertrag = gesamt_ertrag.join(seed_cost, gesamt_ertrag.idx == seed_cost.idx).drop(\"idx\")\n",
        "gesamt_ertrag.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iedwMtJC1RNA"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "gesamt_ertrag = gesamt_ertrag.withColumn(\"revenue_after_water_cost\", func.col(\"revenue\") - (func.col(\"water_consumption\")* 0.02))\n",
        "gesamt_ertrag = gesamt_ertrag.withColumn(\"netto_revenue\",col(\"revenue_after_water_cost\") - col(\"cost_of_seed\"))\n",
        "gesamt_ertrag.orderBy(col(\"netto_revenue\").desc()).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1V2EzJA3J3H"
      },
      "source": [
        "Um auch für das Getreide in unserem Hofladen gewinnbringend vertreiben können, müssen wir Jeremy bei der Preisfindung unterstützen. \n",
        "\n",
        "Die Einnahmen sind bisher in Pence angegeben Rechnenen Sie diese in Pfund um.\n",
        "\n",
        ">> Randinformationen:\n",
        "*   Ein Pfund (£) hat 100 Pence (p).\n",
        "*   Für den Mindestverkaufspreis müssen unsere Getreidesorten noch mit einer Mehrwertsteuer von 19% besteuert werden.\n",
        "* Für einen lukrativen Verkaufspreis müssen wir den Nettopreis mit 59% besteuern\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "+ Verwenden Sie für die Preisfindung Lambda-Funktionen. Welche der beiden Variante Sie verwenden bleibt Ihnen überlassen. \n",
        "\n",
        "+ Ermitteln Sie hierfür den Preis pro Stück und erzeugen hierfür eine neue Spalte in dem Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYjUbd0S3x4Z"
      },
      "outputs": [],
      "source": [
        "rdd2 = gesamt_ertrag.rdd.map(lambda x: (x[\"Crop\"], x[\"netto_revenue\"]*1.19*0.01, x[\"netto_revenue\"]*1.59*0.01)) \n",
        "cost_overview = rdd2.toDF([\"Crop\",\"min_selling_price_per_piece\", \"lucrative_selling_price\"])\n",
        "cost_overview = cost_overview.withColumn(\"min_selling_price_per_piece\", func.round(cost_overview[\"min_selling_price_per_piece\"], 2)).withColumn(\"lucrative_selling_price\", func.round(cost_overview[\"lucrative_selling_price\"], 2))\n",
        "cost_overview.withColumn(\"currency\", func.lit(\"£\")).orderBy(col(\"lucrative_selling_price\").desc()).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1hODm_E6iaN"
      },
      "outputs": [],
      "source": [
        "# By Calling function\n",
        "def calc_func(x):\n",
        "    Crop = x.Crop\n",
        "    Netto_Revenue = x.netto_revenue \n",
        "    min_selling_price_per_piece = Netto_Revenue*1.19*0.01\n",
        "    lucrative_selling_price = Netto_Revenue*1.59*0.01\n",
        "    return (Crop, min_selling_price_per_piece, lucrative_selling_price)\n",
        "\n",
        "rdd_process=gesamt_ertrag.rdd.map(lambda x: calc_func(x))\n",
        "cost_overview_with_func = rdd_process.toDF([\"Crop\",\"min_selling_price_per_piece\", \"lucrative_selling_price\"])\n",
        "cost_overview_with_func = cost_overview_with_func.withColumn(\"min_selling_price_per_piece\", func.round(cost_overview_with_func[\"min_selling_price_per_piece\"], 2)).withColumn(\"lucrative_selling_price\", func.round(cost_overview_with_func[\"lucrative_selling_price\"], 2))\n",
        "cost_overview_with_func.withColumn(\"currency\", func.lit(\"£\")).orderBy(col(\"lucrative_selling_price\").desc()).show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Loesung_Uebungen_Teil3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}